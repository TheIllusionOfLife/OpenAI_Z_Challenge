{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaeological Site Discovery Workflow\n",
    "\n",
    "## OpenAI to Z Challenge: Amazon Rainforest Archaeological Sites\n",
    "\n",
    "This notebook demonstrates the complete workflow for discovering archaeological sites in the Amazon rainforest using:\n",
    "- **LiDAR data** for terrain analysis\n",
    "- **Satellite imagery** for vegetation and land use patterns\n",
    "- **OpenAI models** for archaeological literature analysis\n",
    "- **Machine learning** for site prediction and validation\n",
    "\n",
    "### Workflow Overview\n",
    "1. **Data Loading & Preprocessing**\n",
    "2. **Terrain Feature Extraction**\n",
    "3. **Vegetation Analysis**\n",
    "4. **Literature Mining**\n",
    "5. **Site Detection & Clustering**\n",
    "6. **Validation & Reporting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loading import (\n",
    "    LiDARLoader, SatelliteImageLoader, NDVILoader, \n",
    "    GISDataLoader, ArchaeologicalLiteratureLoader, DatasetValidator\n",
    ")\n",
    "from geospatial_processing import (\n",
    "    CoordinateTransformer, RasterProcessor, TerrainAnalyzer,\n",
    "    VegetationAnalyzer, SpatialFeatureExtractor, ArchaeologicalSiteDetector\n",
    ")\n",
    "from openai_integration import (\n",
    "    OpenAIClient, LiteratureAnalyzer, SiteDescriptionGenerator,\n",
    "    ArchaeologicalKnowledgeExtractor, ModelSelector, TokenManager\n",
    ")\n",
    "from config import Config\n",
    "from logging_utils import setup_logging\n",
    "\n",
    "# Set up configuration and logging\n",
    "config = Config()\n",
    "logger = setup_logging(config.get('log_level', 'INFO'))\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"üìç Working directory: {os.getcwd()}\")\n",
    "print(f\"üîß Configuration loaded: {config.get('environment', 'development')} environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Preprocessing\n",
    "\n",
    "### Simulated Data Generation\n",
    "For demonstration purposes, we'll create synthetic data that represents typical Amazon rainforest geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_amazon_data(size=(500, 500), seed=42):\n",
    "    \"\"\"Create synthetic Amazon rainforest data for demonstration.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create synthetic elevation data (typical Amazon elevation: 0-200m)\n",
    "    x = np.linspace(0, 100, size[0])\n",
    "    y = np.linspace(0, 100, size[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Base elevation with some hills and valleys\n",
    "    elevation = (\n",
    "        50 + \n",
    "        30 * np.sin(X/20) * np.cos(Y/15) +\n",
    "        20 * np.sin(X/10) * np.sin(Y/25) +\n",
    "        np.random.normal(0, 5, size)\n",
    "    )\n",
    "    elevation = np.clip(elevation, 0, 200)\n",
    "    \n",
    "    # Create synthetic NDVI data (Amazon typically 0.6-0.9)\n",
    "    base_ndvi = 0.75\n",
    "    ndvi = (\n",
    "        base_ndvi + \n",
    "        0.1 * np.sin(X/30) * np.cos(Y/20) +\n",
    "        0.05 * np.random.normal(0, 1, size)\n",
    "    )\n",
    "    ndvi = np.clip(ndvi, 0, 1)\n",
    "    \n",
    "    # Add some archaeological \"signatures\" - areas with different patterns\n",
    "    # These represent potential sites with different vegetation/terrain\n",
    "    for _ in range(5):  # Add 5 potential sites\n",
    "        center_x = np.random.randint(50, size[0]-50)\n",
    "        center_y = np.random.randint(50, size[1]-50)\n",
    "        radius = np.random.randint(10, 30)\n",
    "        \n",
    "        # Create circular anomaly\n",
    "        mask = (X - center_x)**2 + (Y - center_y)**2 < radius**2\n",
    "        \n",
    "        # Slightly elevated area (archaeological mounds)\n",
    "        elevation[mask] += np.random.uniform(5, 15)\n",
    "        \n",
    "        # Reduced vegetation (cleared areas)\n",
    "        ndvi[mask] -= np.random.uniform(0.1, 0.3)\n",
    "    \n",
    "    return {\n",
    "        'elevation': elevation.astype(np.float32),\n",
    "        'ndvi': ndvi.astype(np.float32),\n",
    "        'coordinates': (X, Y),\n",
    "        'metadata': {\n",
    "            'crs': 'EPSG:4326',\n",
    "            'bounds': [0, 0, 100, 100],\n",
    "            'resolution': 0.2,  # 200m resolution\n",
    "            'region': 'Amazon Basin'\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"üåç Generating synthetic Amazon rainforest data...\")\n",
    "amazon_data = create_synthetic_amazon_data()\n",
    "\n",
    "elevation_data = amazon_data['elevation']\n",
    "ndvi_data = amazon_data['ndvi']\n",
    "X_coords, Y_coords = amazon_data['coordinates']\n",
    "metadata = amazon_data['metadata']\n",
    "\n",
    "print(f\"‚úÖ Data generated successfully:\")\n",
    "print(f\"   üìè Grid size: {elevation_data.shape}\")\n",
    "print(f\"   üèîÔ∏è  Elevation range: {elevation_data.min():.1f} - {elevation_data.max():.1f}m\")\n",
    "print(f\"   üå± NDVI range: {ndvi_data.min():.3f} - {ndvi_data.max():.3f}\")\n",
    "print(f\"   üó∫Ô∏è  CRS: {metadata['crs']}\")\n",
    "print(f\"   üìê Resolution: {metadata['resolution']}km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the synthetic data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot elevation data\n",
    "im1 = axes[0].imshow(elevation_data, cmap='terrain', aspect='equal')\n",
    "axes[0].set_title('Elevation Data (Amazon Basin)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('X Coordinate (km)')\n",
    "axes[0].set_ylabel('Y Coordinate (km)')\n",
    "plt.colorbar(im1, ax=axes[0], label='Elevation (m)')\n",
    "\n",
    "# Plot NDVI data\n",
    "im2 = axes[1].imshow(ndvi_data, cmap='RdYlGn', aspect='equal', vmin=0, vmax=1)\n",
    "axes[1].set_title('NDVI Data (Vegetation Index)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X Coordinate (km)')\n",
    "axes[1].set_ylabel('Y Coordinate (km)')\n",
    "plt.colorbar(im2, ax=axes[1], label='NDVI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Data visualization complete. Note the circular anomalies that represent potential archaeological sites.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terrain Feature Extraction\n",
    "\n",
    "Extract terrain features that are important for archaeological site detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize terrain analyzer\n",
    "terrain_analyzer = TerrainAnalyzer()\n",
    "pixel_size = metadata['resolution'] * 1000  # Convert km to meters\n",
    "\n",
    "print(f\"üèîÔ∏è  Extracting terrain features (pixel size: {pixel_size}m)...\")\n",
    "\n",
    "# Calculate terrain features\n",
    "slope = terrain_analyzer.calculate_slope(elevation_data, pixel_size)\n",
    "aspect = terrain_analyzer.calculate_aspect(elevation_data, pixel_size)\n",
    "curvature = terrain_analyzer.calculate_curvature(elevation_data, pixel_size)\n",
    "terrain_features = terrain_analyzer.identify_terrain_features(elevation_data)\n",
    "\n",
    "print(f\"‚úÖ Terrain analysis complete:\")\n",
    "print(f\"   üìê Slope range: {slope.min():.1f}¬∞ - {slope.max():.1f}¬∞\")\n",
    "print(f\"   üß≠ Aspect range: {aspect.min():.1f}¬∞ - {aspect.max():.1f}¬∞\")\n",
    "print(f\"   „Ä∞Ô∏è  Profile curvature range: {curvature['profile_curvature'].min():.4f} - {curvature['profile_curvature'].max():.4f}\")\n",
    "print(f\"   üóª Identified {len(terrain_features['peaks'])} peaks\")\n",
    "print(f\"   üèûÔ∏è  Identified {len(terrain_features['valleys'])} valleys\")\n",
    "print(f\"   ‚õ∞Ô∏è  Identified {len(terrain_features['ridges'])} ridges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize terrain features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Slope\n",
    "im1 = axes[0,0].imshow(slope, cmap='YlOrRd', aspect='equal')\n",
    "axes[0,0].set_title('Slope (degrees)', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0,0], shrink=0.8)\n",
    "\n",
    "# Aspect\n",
    "im2 = axes[0,1].imshow(aspect, cmap='hsv', aspect='equal')\n",
    "axes[0,1].set_title('Aspect (degrees)', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[0,1], shrink=0.8)\n",
    "\n",
    "# Profile curvature\n",
    "im3 = axes[1,0].imshow(curvature['profile_curvature'], cmap='RdBu_r', aspect='equal')\n",
    "axes[1,0].set_title('Profile Curvature', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[1,0], shrink=0.8)\n",
    "\n",
    "# Plan curvature\n",
    "im4 = axes[1,1].imshow(curvature['plan_curvature'], cmap='RdBu_r', aspect='equal')\n",
    "axes[1,1].set_title('Plan Curvature', fontweight='bold')\n",
    "plt.colorbar(im4, ax=axes[1,1], shrink=0.8)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Terrain feature visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vegetation Analysis\n",
    "\n",
    "Analyze vegetation patterns to identify potential archaeological signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vegetation analyzer\n",
    "vegetation_analyzer = VegetationAnalyzer()\n",
    "\n",
    "print(\"üå± Analyzing vegetation patterns...\")\n",
    "\n",
    "# Classify vegetation density\n",
    "vegetation_classes = vegetation_analyzer.classify_vegetation_density(ndvi_data)\n",
    "\n",
    "# Detect vegetation anomalies\n",
    "vegetation_anomalies = vegetation_analyzer.detect_vegetation_anomalies(\n",
    "    ndvi_data, window_size=5, threshold=0.1\n",
    ")\n",
    "\n",
    "# Create synthetic NIR and Red bands for additional indices\n",
    "# In real data, these would come from satellite imagery\n",
    "red_band = np.random.uniform(0.1, 0.3, elevation_data.shape)\n",
    "nir_band = ndvi_data * (red_band + ndvi_data)  # Derived from NDVI formula\n",
    "\n",
    "vegetation_indices = vegetation_analyzer.calculate_vegetation_indices(red_band, nir_band)\n",
    "\n",
    "print(f\"‚úÖ Vegetation analysis complete:\")\n",
    "print(f\"   üåø Vegetation classes: {np.unique(vegetation_classes)}\")\n",
    "print(f\"   ‚ö†Ô∏è  Anomalous pixels: {np.sum(vegetation_anomalies)} ({np.sum(vegetation_anomalies)/vegetation_anomalies.size*100:.1f}%)\")\n",
    "print(f\"   üìä NDVI range: {vegetation_indices['ndvi'].min():.3f} - {vegetation_indices['ndvi'].max():.3f}\")\n",
    "print(f\"   üìä SAVI range: {vegetation_indices['savi'].min():.3f} - {vegetation_indices['savi'].max():.3f}\")\n",
    "print(f\"   üìä EVI range: {vegetation_indices['evi'].min():.3f} - {vegetation_indices['evi'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize vegetation analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Vegetation classes\n",
    "class_colors = ['brown', 'orange', 'yellow', 'lightgreen', 'darkgreen']\n",
    "im1 = axes[0,0].imshow(vegetation_classes, cmap='RdYlGn', aspect='equal')\n",
    "axes[0,0].set_title('Vegetation Density Classes', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0,0], shrink=0.8, label='0=Bare, 1=Sparse, 2=Moderate, 3=Dense, 4=Very Dense')\n",
    "\n",
    "# Vegetation anomalies\n",
    "im2 = axes[0,1].imshow(vegetation_anomalies, cmap='Reds', aspect='equal')\n",
    "axes[0,1].set_title('Vegetation Anomalies', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[0,1], shrink=0.8)\n",
    "\n",
    "# SAVI\n",
    "im3 = axes[1,0].imshow(vegetation_indices['savi'], cmap='RdYlGn', aspect='equal')\n",
    "axes[1,0].set_title('SAVI (Soil Adjusted Vegetation Index)', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[1,0], shrink=0.8)\n",
    "\n",
    "# EVI\n",
    "im4 = axes[1,1].imshow(vegetation_indices['evi'], cmap='RdYlGn', aspect='equal')\n",
    "axes[1,1].set_title('EVI (Enhanced Vegetation Index)', fontweight='bold')\n",
    "plt.colorbar(im4, ax=axes[1,1], shrink=0.8)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Vegetation analysis visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Feature Extraction\n",
    "\n",
    "Extract textural and morphological features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spatial feature extractor\n",
    "feature_extractor = SpatialFeatureExtractor()\n",
    "\n",
    "print(\"üîç Extracting spatial features...\")\n",
    "print(\"   This may take a moment for textural analysis...\")\n",
    "\n",
    "# Extract textural features from elevation data\n",
    "textural_features = feature_extractor.extract_textural_features(\n",
    "    elevation_data, window_size=5  # Smaller window for demo\n",
    ")\n",
    "\n",
    "# Create binary image for morphological analysis\n",
    "# Areas with low vegetation could indicate archaeological sites\n",
    "low_vegetation_mask = ndvi_data < 0.6\n",
    "morphological_features = feature_extractor.extract_morphological_features(\n",
    "    low_vegetation_mask\n",
    ")\n",
    "\n",
    "# Create some reference points for distance calculation\n",
    "reference_points = [(25, 25), (75, 75), (25, 75), (75, 25)]  # Corner points\n",
    "distance_features = feature_extractor.calculate_distance_features(\n",
    "    X_coords, Y_coords, reference_points\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Spatial feature extraction complete:\")\n",
    "print(f\"   üèóÔ∏è  Textural features: {list(textural_features.keys())}\")\n",
    "print(f\"   üìê Morphological features: {list(morphological_features.keys())}\")\n",
    "print(f\"   üìè Distance features: {list(distance_features.keys())}\")\n",
    "print(f\"   üéØ Low vegetation area: {morphological_features['area']} pixels\")\n",
    "print(f\"   üîÑ Compactness: {morphological_features['compactness']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Textural features\n",
    "im1 = axes[0,0].imshow(textural_features['contrast'], cmap='viridis', aspect='equal')\n",
    "axes[0,0].set_title('GLCM Contrast', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0,0], shrink=0.8)\n",
    "\n",
    "im2 = axes[0,1].imshow(textural_features['homogeneity'], cmap='viridis', aspect='equal')\n",
    "axes[0,1].set_title('GLCM Homogeneity', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[0,1], shrink=0.8)\n",
    "\n",
    "im3 = axes[0,2].imshow(textural_features['energy'], cmap='viridis', aspect='equal')\n",
    "axes[0,2].set_title('GLCM Energy', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[0,2], shrink=0.8)\n",
    "\n",
    "# Distance features and binary mask\n",
    "im4 = axes[1,0].imshow(distance_features['min_distance'], cmap='plasma', aspect='equal')\n",
    "axes[1,0].set_title('Distance to Nearest Reference', fontweight='bold')\n",
    "plt.colorbar(im4, ax=axes[1,0], shrink=0.8)\n",
    "\n",
    "im5 = axes[1,1].imshow(low_vegetation_mask, cmap='Reds', aspect='equal')\n",
    "axes[1,1].set_title('Low Vegetation Areas', fontweight='bold')\n",
    "plt.colorbar(im5, ax=axes[1,1], shrink=0.8)\n",
    "\n",
    "# Combined feature visualization\n",
    "# Combine multiple features for anomaly detection\n",
    "combined_score = (\n",
    "    (textural_features['contrast'] / textural_features['contrast'].max()) +\n",
    "    (vegetation_anomalies.astype(float)) +\n",
    "    (low_vegetation_mask.astype(float) * 0.5)\n",
    ")\n",
    "im6 = axes[1,2].imshow(combined_score, cmap='hot', aspect='equal')\n",
    "axes[1,2].set_title('Combined Anomaly Score', fontweight='bold')\n",
    "plt.colorbar(im6, ax=axes[1,2], shrink=0.8)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Spatial feature visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Archaeological Site Detection\n",
    "\n",
    "Use our detection algorithms to identify potential archaeological sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize site detector\n",
    "site_detector = ArchaeologicalSiteDetector()\n",
    "\n",
    "print(\"üèõÔ∏è  Detecting potential archaeological sites...\")\n",
    "\n",
    "# Combine all features into a multi-dimensional array\n",
    "feature_stack = np.stack([\n",
    "    elevation_data,\n",
    "    slope,\n",
    "    aspect,\n",
    "    ndvi_data,\n",
    "    textural_features['contrast'],\n",
    "    textural_features['homogeneity'],\n",
    "    vegetation_anomalies.astype(float),\n",
    "    combined_score\n",
    ], axis=2)\n",
    "\n",
    "print(f\"   üìä Feature stack shape: {feature_stack.shape}\")\n",
    "print(f\"   üéØ Features: elevation, slope, aspect, NDVI, contrast, homogeneity, veg_anomalies, combined_score\")\n",
    "\n",
    "# Identify potential sites\n",
    "potential_sites = site_detector.identify_potential_sites(\n",
    "    feature_stack, confidence_threshold=0.6\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Site detection complete:\")\n",
    "print(f\"   üéØ Potential sites found: {len(potential_sites)}\")\n",
    "\n",
    "if potential_sites:\n",
    "    confidences = [site['confidence'] for site in potential_sites]\n",
    "    print(f\"   üìä Confidence range: {min(confidences):.3f} - {max(confidences):.3f}\")\n",
    "    print(f\"   üîù High confidence sites (>0.8): {sum(1 for c in confidences if c > 0.8)}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No sites detected with current threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster nearby detections\n",
    "if potential_sites:\n",
    "    print(\"üîÑ Clustering nearby site detections...\")\n",
    "    \n",
    "    clustered_sites = site_detector.cluster_nearby_detections(\n",
    "        potential_sites, max_distance=15  # 15 pixel radius\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Clustering complete:\")\n",
    "    print(f\"   üéØ Original detections: {len(potential_sites)}\")\n",
    "    print(f\"   üîó Clustered sites: {len(clustered_sites)}\")\n",
    "    \n",
    "    # Display cluster information\n",
    "    for i, cluster in enumerate(clustered_sites):\n",
    "        coords = cluster.representative_coordinates\n",
    "        print(f\"   üìç Cluster {i+1}: ({coords[0]:.1f}, {coords[1]:.1f}) - {cluster.member_count} detections, avg confidence: {cluster.average_confidence:.3f}\")\n",
    "else:\n",
    "    clustered_sites = []\n",
    "    print(\"   ‚ö†Ô∏è  No sites to cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize site detections\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot elevation with detected sites\n",
    "axes[0].imshow(elevation_data, cmap='terrain', aspect='equal', alpha=0.7)\n",
    "if potential_sites:\n",
    "    site_x = [site['coordinates'][0] for site in potential_sites]\n",
    "    site_y = [site['coordinates'][1] for site in potential_sites]\n",
    "    site_conf = [site['confidence'] for site in potential_sites]\n",
    "    \n",
    "    scatter = axes[0].scatter(site_x, site_y, c=site_conf, cmap='Reds', \n",
    "                             s=100, edgecolors='black', linewidth=2, alpha=0.8)\n",
    "    plt.colorbar(scatter, ax=axes[0], label='Confidence')\n",
    "\n",
    "axes[0].set_title('Detected Archaeological Sites\\n(on Elevation)', fontweight='bold')\n",
    "axes[0].set_xlabel('X Coordinate')\n",
    "axes[0].set_ylabel('Y Coordinate')\n",
    "\n",
    "# Plot combined anomaly score with clustered sites\n",
    "axes[1].imshow(combined_score, cmap='hot', aspect='equal', alpha=0.7)\n",
    "if clustered_sites:\n",
    "    cluster_x = [cluster.representative_coordinates[0] for cluster in clustered_sites]\n",
    "    cluster_y = [cluster.representative_coordinates[1] for cluster in clustered_sites]\n",
    "    cluster_conf = [cluster.average_confidence for cluster in clustered_sites]\n",
    "    cluster_size = [cluster.member_count for cluster in clustered_sites]\n",
    "    \n",
    "    scatter2 = axes[1].scatter(cluster_x, cluster_y, c=cluster_conf, cmap='Reds',\n",
    "                              s=[s*50 for s in cluster_size], edgecolors='white', \n",
    "                              linewidth=3, alpha=0.9)\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Avg Confidence')\n",
    "    \n",
    "    # Add cluster labels\n",
    "    for i, (x, y) in enumerate(zip(cluster_x, cluster_y)):\n",
    "        axes[1].annotate(f'C{i+1}', (x, y), xytext=(5, 5), textcoords='offset points',\n",
    "                        fontweight='bold', color='white', fontsize=10)\n",
    "\n",
    "axes[1].set_title('Clustered Sites\\n(on Anomaly Score)', fontweight='bold')\n",
    "axes[1].set_xlabel('X Coordinate')\n",
    "axes[1].set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Site detection visualization complete.\")\n",
    "print(\"   üîç Red points show detected sites with confidence-based coloring\")\n",
    "print(\"   üìè Circle size in right plot indicates number of detections in cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Site Validation & Reporting\n",
    "\n",
    "Validate detected sites and generate reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate detected sites\n",
    "if clustered_sites:\n",
    "    print(\"‚úÖ Validating detected archaeological sites...\")\n",
    "    \n",
    "    validated_sites = []\n",
    "    \n",
    "    for i, cluster in enumerate(clustered_sites):\n",
    "        x, y = cluster.representative_coordinates\n",
    "        \n",
    "        # Get site characteristics\n",
    "        site_elevation = elevation_data[int(y), int(x)]\n",
    "        site_slope = slope[int(y), int(x)]\n",
    "        site_ndvi = ndvi_data[int(y), int(x)]\n",
    "        \n",
    "        # Create site data for validation\n",
    "        site_data = {\n",
    "            'elevation': float(site_elevation),\n",
    "            'slope': float(site_slope),\n",
    "            'distance_to_water': np.random.uniform(200, 800),  # Simulated\n",
    "            'ndvi': float(site_ndvi),\n",
    "            'confidence': cluster.average_confidence\n",
    "        }\n",
    "        \n",
    "        # Validate site characteristics\n",
    "        validation_result = site_detector.validate_site_characteristics(site_data)\n",
    "        \n",
    "        # Generate site report\n",
    "        site_info = {\n",
    "            'coordinates': cluster.representative_coordinates,\n",
    "            'confidence': cluster.average_confidence,\n",
    "            'features': {\n",
    "                'elevation': site_elevation,\n",
    "                'slope': site_slope,\n",
    "                'ndvi': site_ndvi,\n",
    "                'terrain_features': ['elevated_area', 'vegetation_anomaly']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        site_report = site_detector.generate_site_report(site_info)\n",
    "        \n",
    "        validated_site = {\n",
    "            'cluster_id': i + 1,\n",
    "            'coordinates': cluster.representative_coordinates,\n",
    "            'member_count': cluster.member_count,\n",
    "            'confidence': cluster.average_confidence,\n",
    "            'characteristics': site_data,\n",
    "            'validation': validation_result,\n",
    "            'report': site_report\n",
    "        }\n",
    "        \n",
    "        validated_sites.append(validated_site)\n",
    "        \n",
    "        print(f\"\\nüìç Site C{i+1} Validation:\")\n",
    "        print(f\"   üó∫Ô∏è  Coordinates: ({x:.1f}, {y:.1f})\")\n",
    "        print(f\"   üèîÔ∏è  Elevation: {site_elevation:.1f}m\")\n",
    "        print(f\"   üìê Slope: {site_slope:.1f}¬∞\")\n",
    "        print(f\"   üå± NDVI: {site_ndvi:.3f}\")\n",
    "        print(f\"   ‚úÖ Valid: {validation_result['is_valid']}\")\n",
    "        print(f\"   üìä Validation Score: {validation_result['confidence_score']:.3f}\")\n",
    "        if validation_result['reasons']:\n",
    "            print(f\"   ‚ö†Ô∏è  Issues: {', '.join(validation_result['reasons'])}\")\n",
    "\nelse:\n",
    "    validated_sites = []\n",
    "    print(\"‚ö†Ô∏è  No sites to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. OpenAI Literature Analysis (Demonstration)\n",
    "\n",
    "Demonstrate how to use OpenAI models for archaeological literature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires OpenAI API key for actual execution\n",
    "# For demonstration, we'll show the setup and mock responses\n",
    "\n",
    "print(\"üìö OpenAI Literature Analysis Setup...\")\n",
    "\n",
    "# Check if OpenAI API key is available\n",
    "api_key_available = bool(os.getenv('OPENAI_API_KEY'))\n",
    "print(f\"   üîë OpenAI API Key: {'Available' if api_key_available else 'Not Available'}\")\n",
    "\n",
    "if api_key_available:\n",
    "    try:\n",
    "        # Initialize OpenAI client\n",
    "        openai_client = OpenAIClient()\n",
    "        literature_analyzer = LiteratureAnalyzer(openai_client)\n",
    "        site_generator = SiteDescriptionGenerator(openai_client)\n",
    "        \n",
    "        print(\"   ‚úÖ OpenAI client initialized successfully\")\n",
    "        \n",
    "        # Example literature text for analysis\n",
    "        sample_literature = \"\"\"\n",
    "        Recent archaeological surveys in the upper Amazon basin have identified \n",
    "        several pre-Columbian settlement sites near coordinates -3.2¬∞, -60.25¬∞. \n",
    "        The sites show evidence of terra preta (Amazonian dark earth) and \n",
    "        ceramic artifacts dating to approximately 800-1200 CE. Elevated areas \n",
    "        with moderate slopes appear to have been preferred for habitation.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nüìñ Sample Literature Analysis:\")\n",
    "        print(\"   üìÑ Analyzing sample archaeological text...\")\n",
    "        \n",
    "        # Extract coordinates\n",
    "        extracted_coords = literature_analyzer.extract_site_coordinates(sample_literature)\n",
    "        print(f\"   üìç Extracted coordinates: {len(extracted_coords)} sites\")\n",
    "        \n",
    "        # Analyze site descriptions\n",
    "        site_analysis = literature_analyzer.analyze_site_descriptions(sample_literature)\n",
    "        print(f\"   üîç Site analysis complete: {bool(site_analysis)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  OpenAI client error: {e}\")\n",
    "        api_key_available = False\n",
    "\n",
    "if not api_key_available:\n",
    "    print(\"\\nüìù Mock Literature Analysis (No API Key):\")\n",
    "    print(\"   To use OpenAI integration, set OPENAI_API_KEY environment variable\")\n",
    "    \n",
    "    # Mock extracted coordinates\n",
    "    extracted_coords = [\n",
    "        {'latitude': -3.2, 'longitude': -60.25, 'confidence': 0.9},\n",
    "        {'latitude': -3.5, 'longitude': -61.2, 'confidence': 0.8}\n",
    "    ]\n",
    "    \n",
    "    # Mock site analysis\n",
    "    site_analysis = {\n",
    "        'site_type': 'settlement',\n",
    "        'features': ['terra_preta', 'ceramic_artifacts', 'elevated_areas'],\n",
    "        'period': '800-1200 CE',\n",
    "        'cultural_indicators': ['pottery', 'dark_earth'],\n",
    "        'significance': 'high'\n",
    "    }\n",
    "    \n",
    "    print(f\"   üìç Mock extracted coordinates: {len(extracted_coords)} sites\")\n",
    "    print(f\"   üîç Mock site analysis: {site_analysis['site_type']} from {site_analysis['period']}\")\n",
    "\n",
    "print(\"\\nüìö Literature analysis capabilities demonstrated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Results Summary\n",
    "\n",
    "Summarize all findings and create final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "print(\"üìã ARCHAEOLOGICAL SITE DISCOVERY SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüåç STUDY AREA:\")\n",
    "print(f\"   üìç Region: {metadata['region']}\")\n",
    "print(f\"   üìè Grid Size: {elevation_data.shape[0]} x {elevation_data.shape[1]} pixels\")\n",
    "print(f\"   üéØ Resolution: {metadata['resolution']} km/pixel\")\n",
    "print(f\"   üó∫Ô∏è  Coverage: {metadata['bounds'][2] - metadata['bounds'][0]:.1f} x {metadata['bounds'][3] - metadata['bounds'][1]:.1f} km\")\n",
    "\n",
    "print(\"\\nüìä DATA ANALYSIS:\")\n",
    "print(f\"   üèîÔ∏è  Elevation range: {elevation_data.min():.1f} - {elevation_data.max():.1f} m\")\n",
    "print(f\"   üìê Slope range: {slope.min():.1f} - {slope.max():.1f}¬∞\")\n",
    "print(f\"   üå± NDVI range: {ndvi_data.min():.3f} - {ndvi_data.max():.3f}\")\n",
    "print(f\"   üîç Vegetation anomalies: {np.sum(vegetation_anomalies)} pixels ({np.sum(vegetation_anomalies)/vegetation_anomalies.size*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüéØ SITE DETECTION RESULTS:\")\n",
    "print(f\"   üîç Initial detections: {len(potential_sites)}\")\n",
    "print(f\"   üîó Clustered sites: {len(clustered_sites)}\")\n",
    "print(f\"   ‚úÖ Validated sites: {len([s for s in validated_sites if s['validation']['is_valid']])}\")\n",
    "\n",
    "if validated_sites:\n",
    "    print(\"\\nüìç SITE DETAILS:\")\n",
    "    for site in validated_sites:\n",
    "        x, y = site['coordinates']\n",
    "        print(f\"\\n   üèõÔ∏è  Site {site['cluster_id']}:\")\n",
    "        print(f\"      üìç Location: ({x:.1f}, {y:.1f})\")\n",
    "        print(f\"      üéØ Confidence: {site['confidence']:.3f}\")\n",
    "        print(f\"      üë• Detection Count: {site['member_count']}\")\n",
    "        print(f\"      ‚úÖ Valid: {site['validation']['is_valid']}\")\n",
    "        print(f\"      üìä Validation Score: {site['validation']['confidence_score']:.3f}\")\n",
    "        print(f\"      üèîÔ∏è  Elevation: {site['characteristics']['elevation']:.1f}m\")\n",
    "        print(f\"      üìê Slope: {site['characteristics']['slope']:.1f}¬∞\")\n",
    "        print(f\"      üå± NDVI: {site['characteristics']['ndvi']:.3f}\")\n",
    "\n",
    "print(\"\\nüìö LITERATURE ANALYSIS:\")\n",
    "if 'extracted_coords' in locals():\n",
    "    print(f\"   üìñ Literature sites: {len(extracted_coords)}\")\n",
    "    for i, coord in enumerate(extracted_coords):\n",
    "        print(f\"      üìç Lit Site {i+1}: ({coord['latitude']}, {coord['longitude']}) - confidence: {coord['confidence']}\")\n",
    "else:\n",
    "    print(f\"   üìñ Literature analysis: Not performed (no API key)\")\n",
    "\n",
    "print(\"\\nüèÜ RECOMMENDATIONS:\")\n",
    "if validated_sites:\n",
    "    high_confidence_sites = [s for s in validated_sites if s['confidence'] > 0.8]\n",
    "    valid_sites = [s for s in validated_sites if s['validation']['is_valid']]\n",
    "    \n",
    "    print(f\"   üîù High-priority sites for field verification: {len(high_confidence_sites)}\")\n",
    "    print(f\"   ‚úÖ Sites meeting validation criteria: {len(valid_sites)}\")\n",
    "    print(f\"   üìã Next steps:\")\n",
    "    print(f\"      1. High-resolution satellite imagery review\")\n",
    "    print(f\"      2. Detailed LiDAR analysis\")\n",
    "    print(f\"      3. Archaeological literature cross-reference\")\n",
    "    if high_confidence_sites:\n",
    "        print(f\"      4. Field survey for {len(high_confidence_sites)} high-confidence sites\")\n",
    "else:\n",
    "    print(f\"   üìã Adjust detection parameters and re-run analysis\")\n",
    "    print(f\"   üîç Consider different feature combinations\")\n",
    "    print(f\"   üìä Expand study area or improve resolution\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìä WORKFLOW COMPLETED SUCCESSFULLY ‚úÖ\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Export results for further analysis and competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame for export\n",
    "if validated_sites:\n",
    "    results_data = []\n",
    "    \n",
    "    for site in validated_sites:\n",
    "        x, y = site['coordinates']\n",
    "        \n",
    "        result_row = {\n",
    "            'site_id': f\"AMAZON_SITE_{site['cluster_id']:03d}\",\n",
    "            'latitude': y * metadata['resolution'] + metadata['bounds'][1],  # Convert to real coordinates\n",
    "            'longitude': x * metadata['resolution'] + metadata['bounds'][0],\n",
    "            'confidence': site['confidence'],\n",
    "            'detection_count': site['member_count'],\n",
    "            'elevation_m': site['characteristics']['elevation'],\n",
    "            'slope_degrees': site['characteristics']['slope'],\n",
    "            'ndvi': site['characteristics']['ndvi'],\n",
    "            'is_valid': site['validation']['is_valid'],\n",
    "            'validation_score': site['validation']['confidence_score'],\n",
    "            'priority': 'High' if site['confidence'] > 0.8 else 'Medium' if site['confidence'] > 0.6 else 'Low'\n",
    "        }\n",
    "        \n",
    "        results_data.append(result_row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    print(\"üìä Results Export:\")\n",
    "    print(f\"   üìÑ Total sites: {len(results_df)}\")\n",
    "    print(f\"   üîù High priority: {len(results_df[results_df['priority'] == 'High'])}\")\n",
    "    print(f\"   üìã Medium priority: {len(results_df[results_df['priority'] == 'Medium'])}\")\n",
    "    print(f\"   üìù Low priority: {len(results_df[results_df['priority'] == 'Low'])}\")\n",
    "    \n",
    "    # Display results table\n",
    "    print(\"\\nüìã Site Discovery Results:\")\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(results_df.round(3))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = '../results/archaeological_sites_discovered.csv'\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {output_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No validated sites to export\")\n",
    "    results_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\n‚úÖ Archaeological site discovery workflow complete!\")\n",
    "print(\"üéØ This notebook demonstrated the complete pipeline from data loading to site validation.\")\n",
    "print(\"üìä Ready for real competition data integration and model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}