{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models for Archaeological Site Detection\n",
    "\n",
    "## OpenAI to Z Challenge: ML/DL Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates machine learning and deep learning approaches for archaeological site detection:\n",
    "\n",
    "### Models Covered\n",
    "1. **Random Forest Classifier**\n",
    "2. **XGBoost Classifier**\n",
    "3. **Convolutional Neural Network (CNN)**\n",
    "4. **Feature Engineering Pipeline**\n",
    "5. **Model Evaluation and Comparison**\n",
    "6. **Ensemble Methods**\n",
    "\n",
    "### Workflow\n",
    "1. **Data Preparation**\n",
    "2. **Feature Engineering**\n",
    "3. **Traditional ML Models**\n",
    "4. **Deep Learning Models**\n",
    "5. **Model Evaluation**\n",
    "6. **Ensemble & Final Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "# Note: Address specific warnings as they arise rather than suppressing all warnings\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "\n",
    "# Deep Learning (using basic numpy/scipy - can be replaced with TensorFlow/PyTorch)\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from geospatial_processing import (\n",
    "    TerrainAnalyzer, VegetationAnalyzer, SpatialFeatureExtractor\n",
    ")\n",
    "from config import Config\n",
    "\n",
    "# Set up configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"ü§ñ Ready for machine learning model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation & Feature Engineering\n",
    "\n",
    "Create comprehensive training dataset with archaeological site features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_archaeological_training_data(size=(1000, 1000), n_sites=50, seed=42):\n",
    "    \"\"\"Create comprehensive training dataset with known archaeological sites.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    print(f\"üèóÔ∏è  Creating training dataset: {size[0]}x{size[1]} grid with {n_sites} archaeological sites\")\n",
    "    \n",
    "    # Create base environmental data\n",
    "    x = np.linspace(0, 200, size[0])  # 200km x 200km area\n",
    "    y = np.linspace(0, 200, size[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Base elevation (Amazon-like terrain)\n",
    "    elevation = (\n",
    "        100 + \n",
    "        50 * np.sin(X/30) * np.cos(Y/25) +\n",
    "        30 * np.sin(X/15) * np.sin(Y/35) +\n",
    "        np.random.normal(0, 10, size)\n",
    "    )\n",
    "    elevation = np.clip(elevation, 0, 400)\n",
    "    \n",
    "    # Base NDVI (vegetation)\n",
    "    base_ndvi = 0.8\n",
    "    ndvi = (\n",
    "        base_ndvi + \n",
    "        0.15 * np.sin(X/40) * np.cos(Y/30) +\n",
    "        0.1 * np.random.normal(0, 1, size)\n",
    "    )\n",
    "    ndvi = np.clip(ndvi, 0, 1)\n",
    "    \n",
    "    # Initialize site labels\n",
    "    site_labels = np.zeros(size, dtype=int)\n",
    "    site_info = []\n",
    "    \n",
    "    # Generate archaeological sites with realistic characteristics\n",
    "    for site_id in range(n_sites):\n",
    "        # Site location (prefer certain elevations and terrain)\n",
    "        # Archaeological preference: moderate elevation, near water, gentle slopes\n",
    "        \n",
    "        # Weight location selection by elevation preferences\n",
    "        elevation_weight = np.exp(-((elevation - 150)**2) / (2 * 50**2))  # Prefer ~150m elevation\n",
    "        elevation_weight /= elevation_weight.sum()\n",
    "        \n",
    "        # Select location based on weights\n",
    "        flat_idx = np.random.choice(elevation_weight.size, p=elevation_weight.ravel())\n",
    "        center_y, center_x = np.unravel_index(flat_idx, size)\n",
    "        \n",
    "        # Site characteristics\n",
    "        site_type = np.random.choice(['settlement', 'ceremonial', 'agricultural'], \n",
    "                                   p=[0.6, 0.2, 0.2])\n",
    "        \n",
    "        if site_type == 'settlement':\n",
    "            radius = np.random.randint(15, 40)  # Larger settlements\n",
    "            elevation_change = np.random.uniform(3, 12)  # Mounds\n",
    "            ndvi_change = -np.random.uniform(0.15, 0.35)  # Cleared areas\n",
    "        elif site_type == 'ceremonial':\n",
    "            radius = np.random.randint(8, 25)  # Smaller but distinct\n",
    "            elevation_change = np.random.uniform(5, 20)  # Prominent mounds\n",
    "            ndvi_change = -np.random.uniform(0.1, 0.25)  # Partially cleared\n",
    "        else:  # agricultural\n",
    "            radius = np.random.randint(20, 60)  # Large agricultural areas\n",
    "            elevation_change = np.random.uniform(1, 5)  # Subtle terracing\n",
    "            ndvi_change = np.random.uniform(-0.2, 0.1)  # Mixed vegetation\n",
    "        \n",
    "        # Create site footprint\n",
    "        y_indices, x_indices = np.ogrid[:size[0], :size[1]]\n",
    "        mask = (x_indices - center_x)**2 + (y_indices - center_y)**2 <= radius**2\n",
    "        \n",
    "        # Apply archaeological signatures\n",
    "        if mask.any():\n",
    "            # Gradual elevation change (more realistic)\n",
    "            distance_from_center = np.sqrt((x_indices - center_x)**2 + (y_indices - center_y)**2)\n",
    "            elevation_multiplier = np.exp(-distance_from_center / radius) * mask\n",
    "            elevation += elevation_change * elevation_multiplier\n",
    "            \n",
    "            # Vegetation change\n",
    "            ndvi += ndvi_change * elevation_multiplier\n",
    "            \n",
    "            # Mark as archaeological site\n",
    "            site_labels[mask] = 1\n",
    "            \n",
    "            site_info.append({\n",
    "                'site_id': site_id,\n",
    "                'center': (center_x, center_y),\n",
    "                'type': site_type,\n",
    "                'radius': radius,\n",
    "                'elevation_change': elevation_change,\n",
    "                'ndvi_change': ndvi_change\n",
    "            })\n",
    "    \n",
    "    # Clip values to realistic ranges\n",
    "    ndvi = np.clip(ndvi, 0, 1)\n",
    "    \n",
    "    print(f\"‚úÖ Training data created:\")\n",
    "    print(f\"   üèõÔ∏è  Archaeological sites: {len(site_info)}\")\n",
    "    print(f\"   üìç Site pixels: {np.sum(site_labels)} ({np.sum(site_labels)/site_labels.size*100:.2f}%)\")\n",
    "    print(f\"   üèîÔ∏è  Elevation range: {elevation.min():.1f} - {elevation.max():.1f}m\")\n",
    "    print(f\"   üå± NDVI range: {ndvi.min():.3f} - {ndvi.max():.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'elevation': elevation.astype(np.float32),\n",
    "        'ndvi': ndvi.astype(np.float32),\n",
    "        'coordinates': (X, Y),\n",
    "        'site_labels': site_labels,\n",
    "        'site_info': site_info\n",
    "    }\n",
    "\n",
    "# Create training dataset\n",
    "training_data = create_archaeological_training_data(size=(800, 800), n_sites=40)\n",
    "\n",
    "elevation = training_data['elevation']\n",
    "ndvi = training_data['ndvi']\n",
    "X_coords, Y_coords = training_data['coordinates']\n",
    "site_labels = training_data['site_labels']\n",
    "site_info = training_data['site_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elevation with site locations\n",
    "im1 = axes[0].imshow(elevation, cmap='terrain', aspect='equal')\n",
    "axes[0].set_title('Elevation with Archaeological Sites', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], label='Elevation (m)')\n",
    "\n",
    "# Add site markers\n",
    "for site in site_info:\n",
    "    x, y = site['center']\n",
    "    color = {'settlement': 'red', 'ceremonial': 'yellow', 'agricultural': 'blue'}[site['type']]\n",
    "    axes[0].scatter(x, y, c=color, s=50, edgecolors='black', linewidth=1, alpha=0.8)\n",
    "\n",
    "# NDVI\n",
    "im2 = axes[1].imshow(ndvi, cmap='RdYlGn', aspect='equal', vmin=0, vmax=1)\n",
    "axes[1].set_title('NDVI (Vegetation Index)', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], label='NDVI')\n",
    "\n",
    "# Site labels\n",
    "im3 = axes[2].imshow(site_labels, cmap='Reds', aspect='equal')\n",
    "axes[2].set_title('Archaeological Site Labels\\n(Ground Truth)', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2], label='Site (1) / Background (0)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print site type distribution\n",
    "site_types = [site['type'] for site in site_info]\n",
    "type_counts = pd.Series(site_types).value_counts()\n",
    "print(f\"üìä Site Type Distribution:\")\n",
    "for site_type, count in type_counts.items():\n",
    "    print(f\"   {site_type}: {count} sites ({count/len(site_info)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comprehensive features for machine learning\n",
    "print(\"üîç Extracting features for machine learning...\")\n",
    "\n",
    "# Initialize analyzers\n",
    "terrain_analyzer = TerrainAnalyzer()\n",
    "vegetation_analyzer = VegetationAnalyzer()\n",
    "feature_extractor = SpatialFeatureExtractor()\n",
    "\n",
    "# Calculate terrain features\n",
    "pixel_size = 250  # 250m resolution\n",
    "slope = terrain_analyzer.calculate_slope(elevation, pixel_size)\n",
    "aspect = terrain_analyzer.calculate_aspect(elevation, pixel_size)\n",
    "curvature = terrain_analyzer.calculate_curvature(elevation, pixel_size)\n",
    "\n",
    "# Calculate vegetation features\n",
    "vegetation_classes = vegetation_analyzer.classify_vegetation_density(ndvi)\n",
    "vegetation_anomalies = vegetation_analyzer.detect_vegetation_anomalies(ndvi, window_size=3, threshold=0.1)\n",
    "\n",
    "# Calculate spatial features (use smaller window for speed)\n",
    "# Sample a subset for textural analysis to speed up computation\n",
    "step = 4  # Every 4th pixel\n",
    "elevation_sample = elevation[::step, ::step]\n",
    "textural_features_sample = feature_extractor.extract_textural_features(\n",
    "    elevation_sample, window_size=3\n",
    ")\n",
    "\n",
    "# Upsample textural features back to original size\n",
    "from scipy.ndimage import zoom\n",
    "zoom_factor = (elevation.shape[0] / textural_features_sample['contrast'].shape[0],\n",
    "               elevation.shape[1] / textural_features_sample['contrast'].shape[1])\n",
    "\n",
    "textural_features = {}\n",
    "for key, feature in textural_features_sample.items():\n",
    "    textural_features[key] = zoom(feature, zoom_factor, order=1)\n",
    "\n",
    "print(f\"‚úÖ Feature extraction complete:\")\n",
    "print(f\"   üèîÔ∏è  Terrain features: slope, aspect, profile_curvature, plan_curvature\")\n",
    "print(f\"   üå± Vegetation features: classes, anomalies\")\n",
    "print(f\"   üèóÔ∏è  Textural features: {list(textural_features.keys())}\")\n",
    "print(f\"   üìè Feature grid size: {elevation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix for machine learning\n",
    "def create_feature_matrix(elevation, slope, aspect, curvature, ndvi, vegetation_classes, \n",
    "                         vegetation_anomalies, textural_features, site_labels):\n",
    "    \"\"\"Create feature matrix for ML models.\"\"\"\n",
    "    \n",
    "    # Flatten all features to create pixel-wise feature vectors\n",
    "    n_pixels = elevation.size\n",
    "    \n",
    "    features = {\n",
    "        'elevation': elevation.ravel(),\n",
    "        'slope': slope.ravel(),\n",
    "        'aspect': aspect.ravel(),\n",
    "        'profile_curvature': curvature['profile_curvature'].ravel(),\n",
    "        'plan_curvature': curvature['plan_curvature'].ravel(),\n",
    "        'ndvi': ndvi.ravel(),\n",
    "        'vegetation_class': vegetation_classes.ravel(),\n",
    "        'vegetation_anomaly': vegetation_anomalies.ravel().astype(int),\n",
    "        'texture_contrast': textural_features['contrast'].ravel(),\n",
    "        'texture_homogeneity': textural_features['homogeneity'].ravel(),\n",
    "        'texture_energy': textural_features['energy'].ravel(),\n",
    "        'texture_dissimilarity': textural_features['dissimilarity'].ravel()\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_df = pd.DataFrame(features)\n",
    "    \n",
    "    # Add derived features\n",
    "    feature_df['elevation_normalized'] = (feature_df['elevation'] - feature_df['elevation'].mean()) / feature_df['elevation'].std()\n",
    "    feature_df['slope_category'] = pd.cut(feature_df['slope'], bins=[0, 5, 15, 30, 90], \n",
    "                                         labels=['flat', 'gentle', 'moderate', 'steep'])\n",
    "    feature_df['slope_category'] = feature_df['slope_category'].cat.codes\n",
    "    \n",
    "    # Vegetation health indicator\n",
    "    feature_df['vegetation_health'] = feature_df['ndvi'] * (1 - feature_df['vegetation_anomaly'])\n",
    "    \n",
    "    # Terrain roughness (combination of slope and curvature)\n",
    "    feature_df['terrain_roughness'] = np.sqrt(feature_df['slope']**2 + \n",
    "                                             feature_df['profile_curvature']**2 + \n",
    "                                             feature_df['plan_curvature']**2)\n",
    "    \n",
    "    # Archaeological preference score (domain knowledge)\n",
    "    # Sites prefer: moderate elevation, gentle slopes, near water, distinct terrain\n",
    "    feature_df['archaeological_preference'] = (\n",
    "        np.exp(-((feature_df['elevation'] - 150)**2) / (2 * 75**2)) *  # Elevation preference\n",
    "        np.exp(-(feature_df['slope']**2) / (2 * 10**2)) *  # Slope preference\n",
    "        (1 + feature_df['texture_contrast'] / feature_df['texture_contrast'].max())  # Terrain distinctiveness\n",
    "    )\n",
    "    \n",
    "    # Labels\n",
    "    labels = site_labels.ravel()\n",
    "    \n",
    "    return feature_df, labels\n",
    "\n",
    "# Create feature matrix\n",
    "print(\"üìä Creating feature matrix...\")\n",
    "feature_df, labels = create_feature_matrix(\n",
    "    elevation, slope, aspect, curvature, ndvi, vegetation_classes,\n",
    "    vegetation_anomalies, textural_features, site_labels\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Feature matrix created:\")\n",
    "print(f\"   üìè Shape: {feature_df.shape}\")\n",
    "print(f\"   üèõÔ∏è  Positive samples (sites): {np.sum(labels)} ({np.sum(labels)/len(labels)*100:.2f}%)\")\n",
    "print(f\"   üåç Negative samples (background): {len(labels) - np.sum(labels)} ({(len(labels) - np.sum(labels))/len(labels)*100:.2f}%)\")\n",
    "print(f\"   üìã Features: {list(feature_df.columns)}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nüìä Feature Statistics:\")\n",
    "display(feature_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional Machine Learning Models\n",
    "\n",
    "Train and evaluate Random Forest and XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "print(\"ü§ñ Preparing data for machine learning...\")\n",
    "\n",
    "# Handle any infinite values\n",
    "feature_df = feature_df.replace([np.inf, -np.inf], np.nan)\n",
    "feature_df = feature_df.fillna(feature_df.median())\n",
    "\n",
    "# Split features and labels\n",
    "X = feature_df.values\n",
    "y = labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data prepared:\")\n",
    "print(f\"   üìä Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   üìä Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"   üèõÔ∏è  Training positive rate: {np.sum(y_train)/len(y_train)*100:.2f}%\")\n",
    "print(f\"   üèõÔ∏è  Test positive rate: {np.sum(y_test)/len(y_test)*100:.2f}%\")\n",
    "\n",
    "# Feature scaling for some algorithms\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"   ‚öñÔ∏è  Features scaled for algorithms that require normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"üå≤ Training Random Forest model...\")\n",
    "\n",
    "# Random Forest with class balancing\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight='balanced',  # Handle imbalanced classes\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"‚úÖ Random Forest training complete:\")\n",
    "print(f\"   üìä Test AUC: {rf_auc:.4f}\")\n",
    "print(f\"   üìä CV AUC: {rf_cv_scores.mean():.4f} ¬± {rf_cv_scores.std():.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': feature_df.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüîù Top 5 Random Forest Features:\")\n",
    "for idx, row in feature_importance_rf.head().iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìã Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred, target_names=['Background', 'Archaeological Site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"üöÄ Training XGBoost model...\")\n",
    "\n",
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train model with early stopping\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model\n",
    "xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"‚úÖ XGBoost training complete:\")\n",
    "print(f\"   üìä Test AUC: {xgb_auc:.4f}\")\n",
    "print(f\"   üìä CV AUC: {xgb_cv_scores.mean():.4f} ¬± {xgb_cv_scores.std():.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': feature_df.columns,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüîù Top 5 XGBoost Features:\")\n",
    "for idx, row in feature_importance_xgb.head().iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìã XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=['Background', 'Archaeological Site']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Convolutional Neural Network\n",
    "\n",
    "Implement a basic CNN for spatial pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN implementation using numpy/scipy\n",
    "class SimpleCNN:\n",
    "    \"\"\"Simple CNN implementation for archaeological site detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(11, 11, 4)):\n",
    "        \"\"\"Initialize CNN with basic architecture.\"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Define simple filters (edge detection, blob detection)\n",
    "        self.filters = {\n",
    "            'edge_horizontal': np.array([[-1, -1, -1], [2, 2, 2], [-1, -1, -1]]),\n",
    "            'edge_vertical': np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]]),\n",
    "            'blob_detector': np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]),\n",
    "            'gaussian_blur': np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16\n",
    "        }\n",
    "        \n",
    "        # Simple learned weights (in practice, these would be learned)\n",
    "        self.feature_weights = np.random.normal(0, 0.1, (len(self.filters) * input_shape[2] * 2, 1))  # Fixed: Account for both max_response and mean_response\n",
    "        self.bias = 0.0\n",
    "        \n",
    "    def extract_patches(self, image_stack, patch_size=11):\n",
    "        \"\"\"Extract patches from image stack.\"\"\"\n",
    "        h, w, c = image_stack.shape\n",
    "        patches = []\n",
    "        labels = []\n",
    "        \n",
    "        half_patch = patch_size // 2\n",
    "        \n",
    "        # Sample patches (skip edge pixels)\n",
    "        step = 8  # Sample every 8th pixel for speed\n",
    "        for i in range(half_patch, h - half_patch, step):\n",
    "            for j in range(half_patch, w - half_patch, step):\n",
    "                patch = image_stack[i-half_patch:i+half_patch+1, \n",
    "                                  j-half_patch:j+half_patch+1, :]\n",
    "                patches.append(patch)\n",
    "                labels.append(site_labels[i, j])\n",
    "        \n",
    "        return np.array(patches), np.array(labels)\n",
    "    \n",
    "    def convolve_with_filters(self, patch):\n",
    "        \"\"\"Apply convolution filters to patch.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for channel in range(patch.shape[2]):\n",
    "            channel_data = patch[:, :, channel]\n",
    "            \n",
    "            for filter_name, filter_kernel in self.filters.items():\n",
    "                # Apply convolution\n",
    "                conv_result = convolve2d(channel_data, filter_kernel, mode='valid')\n",
    "                \n",
    "                # Pool (take max, mean, etc.)\n",
    "                max_response = np.max(conv_result)\n",
    "                mean_response = np.mean(conv_result)\n",
    "                \n",
    "                features.extend([max_response, mean_response])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def predict_patch(self, patch):\n",
    "        \"\"\"Predict single patch.\"\"\"\n",
    "        features = self.convolve_with_filters(patch)\n",
    "        \n",
    "        # Simple linear classifier\n",
    "        score = np.dot(features[:len(self.feature_weights)], self.feature_weights.ravel()) + self.bias\n",
    "        probability = 1 / (1 + np.exp(-score))  # Sigmoid\n",
    "        \n",
    "        return probability[0] if hasattr(probability, '__len__') else probability\n",
    "    \n",
    "    def fit(self, patches, labels, learning_rate=0.01, epochs=50):\n",
    "        \"\"\"Simple training procedure.\"\"\"\n",
    "        print(f\"üß† Training Simple CNN...\")\n",
    "        print(f\"   üìä Training patches: {len(patches)}\")\n",
    "        print(f\"   üéØ Positive rate: {np.mean(labels)*100:.2f}%\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            \n",
    "            for patch, label in zip(patches, labels):\n",
    "                # Forward pass\n",
    "                features = self.convolve_with_filters(patch)\n",
    "                features = features[:len(self.feature_weights)]  # Ensure size match\n",
    "                \n",
    "                score = np.dot(features, self.feature_weights.ravel()) + self.bias\n",
    "                pred_prob = 1 / (1 + np.exp(-score))\n",
    "                \n",
    "                # Loss (binary cross-entropy)\n",
    "                loss = -(label * np.log(pred_prob + 1e-7) + (1 - label) * np.log(1 - pred_prob + 1e-7))\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Accuracy\n",
    "                pred_class = 1 if pred_prob > 0.5 else 0\n",
    "                if pred_class == label:\n",
    "                    correct += 1\n",
    "                \n",
    "                # Backward pass (simple gradient descent)\n",
    "                error = pred_prob - label\n",
    "                self.feature_weights -= learning_rate * error * features.reshape(-1, 1)\n",
    "                self.bias -= learning_rate * error\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                avg_loss = total_loss / len(patches)\n",
    "                accuracy = correct / len(patches)\n",
    "                print(f\"   Epoch {epoch}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, patches):\n",
    "        \"\"\"Predict multiple patches.\"\"\"\n",
    "        predictions = []\n",
    "        for patch in patches:\n",
    "            pred = self.predict_patch(patch)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Prepare data for CNN\n",
    "print(\"üñºÔ∏è  Preparing data for CNN...\")\n",
    "\n",
    "# Create multi-channel image stack\n",
    "image_stack = np.stack([\n",
    "    elevation,\n",
    "    slope,\n",
    "    ndvi,\n",
    "    textural_features['contrast']\n",
    "], axis=2)\n",
    "\n",
    "# Normalize each channel\n",
    "for i in range(image_stack.shape[2]):\n",
    "    channel = image_stack[:, :, i]\n",
    "    channel_min, channel_max = channel.min(), channel.max()\n",
    "    if channel_max > channel_min:\n",
    "        image_stack[:, :, i] = (channel - channel_min) / (channel_max - channel_min)\n",
    "\n",
    "# Extract patches\n",
    "cnn = SimpleCNN(input_shape=(11, 11, 4))\n",
    "patches, patch_labels = cnn.extract_patches(image_stack, patch_size=11)\n",
    "\n",
    "print(f\"‚úÖ CNN data prepared:\")\n",
    "print(f\"   üìä Total patches: {len(patches)}\")\n",
    "print(f\"   üìè Patch shape: {patches[0].shape}\")\n",
    "print(f\"   üéØ Positive patches: {np.sum(patch_labels)} ({np.mean(patch_labels)*100:.2f}%)\")\n",
    "\n",
    "# Split patches for training/testing\n",
    "patch_train, patch_test, label_train, label_test = train_test_split(\n",
    "    patches, patch_labels, test_size=0.2, random_state=42, stratify=patch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "cnn.fit(patch_train, label_train, learning_rate=0.001, epochs=30)\n",
    "\n",
    "# Make predictions\n",
    "cnn_pred_proba = cnn.predict(patch_test)\n",
    "cnn_pred = (cnn_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_auc = roc_auc_score(label_test, cnn_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úÖ Simple CNN training complete:\")\n",
    "print(f\"   üìä Test AUC: {cnn_auc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìã Simple CNN Classification Report:\")\n",
    "print(classification_report(label_test, cnn_pred, target_names=['Background', 'Archaeological Site']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Visualization\n",
    "\n",
    "Compare all models and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models_summary = {\n",
    "    'Random Forest': {\n",
    "        'AUC': rf_auc,\n",
    "        'CV_AUC_mean': rf_cv_scores.mean(),\n",
    "        'CV_AUC_std': rf_cv_scores.std()\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'AUC': xgb_auc,\n",
    "        'CV_AUC_mean': xgb_cv_scores.mean(),\n",
    "        'CV_AUC_std': xgb_cv_scores.std()\n",
    "    },\n",
    "    'Simple CNN': {\n",
    "        'AUC': cnn_auc,\n",
    "        'CV_AUC_mean': cnn_auc,  # No CV for CNN in this demo\n",
    "        'CV_AUC_std': 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, metrics in models_summary.items():\n",
    "    print(f\"\\nü§ñ {model_name}:\")\n",
    "    print(f\"   üìä Test AUC: {metrics['AUC']:.4f}\")\n",
    "    print(f\"   üìä CV AUC: {metrics['CV_AUC_mean']:.4f} ¬± {metrics['CV_AUC_std']:.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_model = max(models_summary.keys(), key=lambda x: models_summary[x]['AUC'])\n",
    "print(f\"\\nüèÜ Best performing model: {best_model} (AUC: {models_summary[best_model]['AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ROC Curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_pred_proba)\n",
    "fpr_cnn, tpr_cnn, _ = roc_curve(label_test, cnn_pred_proba)\n",
    "\n",
    "axes[0,0].plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={rf_auc:.3f})', linewidth=2)\n",
    "axes[0,0].plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={xgb_auc:.3f})', linewidth=2)\n",
    "axes[0,0].plot(fpr_cnn, tpr_cnn, label=f'Simple CNN (AUC={cnn_auc:.3f})', linewidth=2)\n",
    "axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[0,0].set_xlabel('False Positive Rate')\n",
    "axes[0,0].set_ylabel('True Positive Rate')\n",
    "axes[0,0].set_title('ROC Curves Comparison', fontweight='bold')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature Importance Comparison (RF vs XGBoost)\n",
    "comparison_features = ['elevation', 'slope', 'ndvi', 'archaeological_preference', 'terrain_roughness']\n",
    "rf_importance = [feature_importance_rf[feature_importance_rf['feature'] == f]['importance'].iloc[0] \n",
    "                if f in feature_importance_rf['feature'].values else 0 for f in comparison_features]\n",
    "xgb_importance = [feature_importance_xgb[feature_importance_xgb['feature'] == f]['importance'].iloc[0] \n",
    "                 if f in feature_importance_xgb['feature'].values else 0 for f in comparison_features]\n",
    "\n",
    "x = np.arange(len(comparison_features))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,1].bar(x - width/2, rf_importance, width, label='Random Forest', alpha=0.8)\n",
    "axes[0,1].bar(x + width/2, xgb_importance, width, label='XGBoost', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Features')\n",
    "axes[0,1].set_ylabel('Importance')\n",
    "axes[0,1].set_title('Feature Importance Comparison', fontweight='bold')\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(comparison_features, rotation=45, ha='right')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction Distribution\n",
    "axes[1,0].hist(rf_pred_proba[y_test == 0], bins=50, alpha=0.5, label='Background', density=True)\n",
    "axes[1,0].hist(rf_pred_proba[y_test == 1], bins=50, alpha=0.5, label='Archaeological Sites', density=True)\n",
    "axes[1,0].set_xlabel('Predicted Probability')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Random Forest Prediction Distribution', fontweight='bold')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model AUC Comparison\n",
    "model_names = ['Random Forest', 'XGBoost', 'Simple CNN']\n",
    "auc_scores = [rf_auc, xgb_auc, cnn_auc]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = axes[1,1].bar(model_names, auc_scores, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[1,1].set_ylabel('AUC Score')\n",
    "axes[1,1].set_title('Model Performance Comparison', fontweight='bold')\n",
    "axes[1,1].set_ylim([0.5, 1.0])\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, auc_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Model performance visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Model and Final Predictions\n",
    "\n",
    "Combine models for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions\n",
    "print(\"üéØ Creating ensemble model...\")\n",
    "\n",
    "# Simple ensemble: weighted average of predictions\n",
    "# Weight by individual model AUC performance\n",
    "total_auc = rf_auc + xgb_auc + cnn_auc\n",
    "rf_weight = rf_auc / total_auc\n",
    "xgb_weight = xgb_auc / total_auc\n",
    "cnn_weight = cnn_auc / total_auc\n",
    "\n",
    "print(f\"   ‚öñÔ∏è  Model weights:\")\n",
    "print(f\"      Random Forest: {rf_weight:.3f}\")\n",
    "print(f\"      XGBoost: {xgb_weight:.3f}\")\n",
    "print(f\"      Simple CNN: {cnn_weight:.3f}\")\n",
    "\n",
    "# For ensemble, we need to align predictions\n",
    "# Use RF and XGBoost predictions (same test set)\n",
    "ensemble_pred_proba = (rf_weight * rf_pred_proba + xgb_weight * xgb_pred_proba) / (rf_weight + xgb_weight)\n",
    "ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_pred_proba)\n",
    "\n",
    "print(f\"\\n‚úÖ Ensemble model created:\")\n",
    "print(f\"   üìä Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "print(f\"   üìà Improvement over best single model: {ensemble_auc - max(rf_auc, xgb_auc):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìã Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, ensemble_pred, target_names=['Background', 'Archaeological Site']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best model to full dataset for site predictions\n",
    "print(\"üó∫Ô∏è  Applying best model to full dataset...\")\n",
    "\n",
    "# Use the ensemble model for final predictions\n",
    "full_rf_pred = rf_model.predict_proba(X)[:, 1]\n",
    "full_xgb_pred = xgb_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Create ensemble prediction for full dataset\n",
    "full_ensemble_pred = (rf_weight * full_rf_pred + xgb_weight * full_xgb_pred) / (rf_weight + xgb_weight)\n",
    "\n",
    "# Reshape predictions back to spatial grid\n",
    "prediction_map = full_ensemble_pred.reshape(elevation.shape)\n",
    "\n",
    "# Find high-confidence predictions\n",
    "high_confidence_threshold = 0.8\n",
    "high_confidence_sites = prediction_map > high_confidence_threshold\n",
    "\n",
    "print(f\"‚úÖ Full dataset predictions complete:\")\n",
    "print(f\"   üìä Prediction range: {prediction_map.min():.3f} - {prediction_map.max():.3f}\")\n",
    "print(f\"   üéØ High confidence pixels (>{high_confidence_threshold}): {np.sum(high_confidence_sites)}\")\n",
    "print(f\"   üìç High confidence area: {np.sum(high_confidence_sites)/high_confidence_sites.size*100:.3f}% of total area\")\n",
    "\n",
    "# Compare with ground truth\n",
    "true_positive_rate = np.sum((prediction_map > 0.5) & (site_labels == 1)) / np.sum(site_labels == 1)\n",
    "false_positive_rate = np.sum((prediction_map > 0.5) & (site_labels == 0)) / np.sum(site_labels == 0)\n",
    "\n",
    "print(f\"\\nüìä Full Dataset Performance:\")\n",
    "print(f\"   ‚úÖ True Positive Rate: {true_positive_rate:.3f}\")\n",
    "print(f\"   ‚ùå False Positive Rate: {false_positive_rate:.3f}\")\n",
    "print(f\"   üìä Overall AUC on full data: {roc_auc_score(site_labels.ravel(), full_ensemble_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Ground truth sites\n",
    "im1 = axes[0,0].imshow(site_labels, cmap='Reds', aspect='equal')\n",
    "axes[0,0].set_title('Ground Truth Archaeological Sites', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0,0], shrink=0.8)\n",
    "\n",
    "# Ensemble predictions\n",
    "im2 = axes[0,1].imshow(prediction_map, cmap='hot', aspect='equal', vmin=0, vmax=1)\n",
    "axes[0,1].set_title('Ensemble Model Predictions', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[0,1], label='Probability', shrink=0.8)\n",
    "\n",
    "# High confidence sites\n",
    "im3 = axes[1,0].imshow(high_confidence_sites, cmap='Reds', aspect='equal')\n",
    "axes[1,0].set_title(f'High Confidence Sites (>{high_confidence_threshold})', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[1,0], shrink=0.8)\n",
    "\n",
    "# Overlay: predictions on elevation\n",
    "axes[1,1].imshow(elevation, cmap='terrain', aspect='equal', alpha=0.7)\n",
    "overlay = axes[1,1].imshow(prediction_map, cmap='hot', aspect='equal', alpha=0.5, vmin=0, vmax=1)\n",
    "axes[1,1].set_title('Predictions Overlaid on Elevation', fontweight='bold')\n",
    "plt.colorbar(overlay, ax=axes[1,1], label='Probability', shrink=0.8)\n",
    "\n",
    "# Add true site markers to overlay\n",
    "for site in site_info:\n",
    "    x, y = site['center']\n",
    "    axes[1,1].scatter(x, y, c='white', s=30, edgecolors='black', linewidth=2, marker='o')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('X Coordinate')\n",
    "    ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Final prediction visualization complete.\")\n",
    "print(\"   ‚≠ï White circles in bottom-right plot show true archaeological sites\")\n",
    "print(\"   üî• Hot colors indicate high probability of archaeological sites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Summary and Export\n",
    "\n",
    "Summarize model performance and prepare for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model summary\n",
    "print(\"üèÜ MACHINE LEARNING MODEL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"   üìè Study area: {elevation.shape[0]} x {elevation.shape[1]} pixels\")\n",
    "print(f\"   üéØ Total archaeological sites: {len(site_info)}\")\n",
    "print(f\"   üìç Site coverage: {np.sum(site_labels)/site_labels.size*100:.2f}% of area\")\n",
    "print(f\"   üìã Features extracted: {feature_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nü§ñ MODEL PERFORMANCE:\")\n",
    "for model_name, metrics in models_summary.items():\n",
    "    print(f\"   {model_name}: AUC = {metrics['AUC']:.4f}\")\n",
    "print(f\"   üéØ Ensemble Model: AUC = {ensemble_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nüîù TOP FEATURES (Random Forest):\")\n",
    "for idx, row in feature_importance_rf.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è  PREDICTION RESULTS:\")\n",
    "print(f\"   üéØ High confidence detections: {np.sum(high_confidence_sites)}\")\n",
    "print(f\"   ‚úÖ True positive rate: {true_positive_rate:.3f}\")\n",
    "print(f\"   ‚ùå False positive rate: {false_positive_rate:.3f}\")\n",
    "\n",
    "# Find predicted site locations\n",
    "from scipy import ndimage\n",
    "from skimage import measure\n",
    "\n",
    "# Label connected components in high confidence predictions\n",
    "labeled_sites, num_sites = ndimage.label(high_confidence_sites)\n",
    "site_properties = measure.regionprops(labeled_sites)\n",
    "\n",
    "predicted_sites = []\n",
    "for i, props in enumerate(site_properties):\n",
    "    if props.area >= 5:  # Minimum site size\n",
    "        centroid_y, centroid_x = props.centroid\n",
    "        predicted_sites.append({\n",
    "            'site_id': f'ML_SITE_{i+1:03d}',\n",
    "            'coordinates': (centroid_x, centroid_y),\n",
    "            'area_pixels': props.area,\n",
    "            'confidence': prediction_map[int(centroid_y), int(centroid_x)],\n",
    "            'bbox': props.bbox\n",
    "        })\n",
    "\n",
    "print(f\"\\nüìç DISCOVERED SITES:\")\n",
    "print(f\"   üéØ Total discovered sites: {len(predicted_sites)}\")\n",
    "for site in predicted_sites[:5]:  # Show first 5\n",
    "    x, y = site['coordinates']\n",
    "    print(f\"   {site['site_id']}: ({x:.1f}, {y:.1f}) - Confidence: {site['confidence']:.3f}, Area: {site['area_pixels']} pixels\")\n",
    "if len(predicted_sites) > 5:\n",
    "    print(f\"   ... and {len(predicted_sites) - 5} more sites\")\n",
    "\n",
    "print(f\"\\nüöÄ RECOMMENDATIONS:\")\n",
    "print(f\"   1. Focus field verification on {len([s for s in predicted_sites if s['confidence'] > 0.9])} highest confidence sites\")\n",
    "print(f\"   2. Cross-reference predictions with archaeological literature\")\n",
    "print(f\"   3. Acquire high-resolution imagery for detailed analysis\")\n",
    "print(f\"   4. Consider ensemble of multiple model architectures\")\n",
    "print(f\"   5. Incorporate additional data sources (soil composition, hydrology)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéØ MACHINE LEARNING WORKFLOW COMPLETED SUCCESSFULLY ‚úÖ\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results for competition submission\n",
    "results_data = []\n",
    "\n",
    "for site in predicted_sites:\n",
    "    x, y = site['coordinates']\n",
    "    \n",
    "    # Convert pixel coordinates to geographic coordinates (mock conversion)\n",
    "    # In real implementation, use proper coordinate transformation\n",
    "    latitude = -3.0 - (y / elevation.shape[0]) * 2.0  # Amazon region\n",
    "    longitude = -60.0 - (x / elevation.shape[1]) * 2.0\n",
    "    \n",
    "    result_row = {\n",
    "        'site_id': site['site_id'],\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'confidence_score': site['confidence'],\n",
    "        'detection_method': 'ML_Ensemble',\n",
    "        'area_pixels': site['area_pixels'],\n",
    "        'elevation_m': elevation[int(y), int(x)],\n",
    "        'slope_degrees': slope[int(y), int(x)],\n",
    "        'ndvi': ndvi[int(y), int(x)],\n",
    "        'priority': 'High' if site['confidence'] > 0.9 else 'Medium' if site['confidence'] > 0.8 else 'Low'\n",
    "    }\n",
    "    \n",
    "    results_data.append(result_row)\n",
    "\n",
    "# Create results DataFrame\n",
    "ml_results_df = pd.DataFrame(results_data)\n",
    "\n",
    "if len(ml_results_df) > 0:\n",
    "    print(f\"üìä ML Results Export:\")\n",
    "    print(f\"   üìÑ Total discovered sites: {len(ml_results_df)}\")\n",
    "    print(f\"   üîù High priority: {len(ml_results_df[ml_results_df['priority'] == 'High'])}\")\n",
    "    print(f\"   üìã Medium priority: {len(ml_results_df[ml_results_df['priority'] == 'Medium'])}\")\n",
    "    print(f\"   üìù Low priority: {len(ml_results_df[ml_results_df['priority'] == 'Low'])}\")\n",
    "    \n",
    "    # Display top results\n",
    "    print(f\"\\nüèÜ Top 10 ML Discovered Sites:\")\n",
    "    top_sites = ml_results_df.nlargest(10, 'confidence_score')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(top_sites[['site_id', 'latitude', 'longitude', 'confidence_score', 'priority']].round(4))\n",
    "    \n",
    "    # Save results\n",
    "    output_path = '../results/ml_archaeological_sites.csv'\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "    ml_results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ ML results saved to: {output_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No sites detected by ML models\")\n",
    "\n",
    "print(\"\\n‚úÖ Machine learning workflow completed successfully!\")\n",
    "print(\"üéØ Ready for integration with competition submission pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.9.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}