{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration Pipeline for Kaggle Competition\n",
    "\n",
    "## OpenAI to Z Challenge: Real Data Integration\n",
    "\n",
    "This notebook demonstrates how to integrate real competition data from the Kaggle OpenAI to Z Challenge:\n",
    "\n",
    "### Data Sources\n",
    "1. **LiDAR Data**: Point cloud data in .las/.laz or GeoTIFF format\n",
    "2. **Satellite Imagery**: High-resolution imagery (GeoTIFF, JPEG2000)\n",
    "3. **NDVI Data**: Vegetation indices for land cover analysis\n",
    "4. **GIS Data**: Terrain, land use, and archaeological context\n",
    "5. **Archaeological Literature**: Research papers with DOI references\n",
    "\n",
    "### Pipeline Stages\n",
    "1. **Data Discovery & Download**\n",
    "2. **Format Validation & Standardization**\n",
    "3. **Coordinate System Alignment**\n",
    "4. **Quality Assessment & Cleaning**\n",
    "5. **Feature Extraction Pipeline**\n",
    "6. **Integration with ML Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "# Note: Address specific warnings as they arise rather than suppressing all warnings\n",
    "\n",
    "# Geospatial libraries\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import calculate_default_transform, reproject\n",
    "import fiona\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loading import (\n",
    "    LiDARLoader, SatelliteImageLoader, NDVILoader, \n",
    "    GISDataLoader, ArchaeologicalLiteratureLoader, DatasetValidator\n",
    ")\n",
    "from geospatial_processing import (\n",
    "    CoordinateTransformer, RasterProcessor\n",
    ")\n",
    "from config import Config\n",
    "from logging_utils import setup_logging\n",
    "\n",
    "# Configuration\n",
    "config = Config()\n",
    "logger = setup_logging('INFO')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸŒ Ready for real geospatial data integration!\")\n",
    "print(f\"ğŸ“ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Discovery & Setup\n",
    "\n",
    "Set up directory structure and check for competition data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data directory structure\n",
    "def setup_data_directories():\n",
    "    \"\"\"Create standardized directory structure for competition data.\"\"\"\n",
    "    \n",
    "    base_dir = Path('../data')\n",
    "    directories = {\n",
    "        'raw': base_dir / 'raw',\n",
    "        'processed': base_dir / 'processed',\n",
    "        'lidar': base_dir / 'raw' / 'lidar',\n",
    "        'satellite': base_dir / 'raw' / 'satellite',\n",
    "        'ndvi': base_dir / 'raw' / 'ndvi',\n",
    "        'gis': base_dir / 'raw' / 'gis',\n",
    "        'literature': base_dir / 'raw' / 'literature',\n",
    "        'aligned': base_dir / 'processed' / 'aligned',\n",
    "        'features': base_dir / 'processed' / 'features',\n",
    "        'models': base_dir / 'processed' / 'models'\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“ Setting up data directory structure...\")\n",
    "    for name, path in directories.items():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ğŸ“‚ {name}: {path}\")\n",
    "    \n",
    "    return directories\n",
    "\n",
    "# Create directories\n",
    "data_dirs = setup_data_directories()\n",
    "\n",
    "print(f\"\\nâœ… Directory structure created successfully!\")\n",
    "print(f\"ğŸ“‹ Ready to receive competition data in standardized format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition data checker\n",
    "def check_competition_data_availability():\n",
    "    \"\"\"Check for available competition data and provide guidance.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” Checking for Kaggle competition data...\")\n",
    "    \n",
    "    # Expected file patterns for competition data\n",
    "    expected_files = {\n",
    "        'LiDAR': ['*.tif', '*.las', '*.laz'],\n",
    "        'Satellite': ['*.tif', '*.jp2', '*.tiff'],\n",
    "        'NDVI': ['*ndvi*.tif', '*vegetation*.tif'],\n",
    "        'GIS': ['*.shp', '*.geojson', '*.gpkg'],\n",
    "        'Literature': ['*.json', '*.csv', '*.txt']\n",
    "    }\n",
    "    \n",
    "    data_status = {}\n",
    "    \n",
    "    for data_type, patterns in expected_files.items():\n",
    "        data_dir = data_dirs['raw'] / data_type.lower()\n",
    "        found_files = []\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            found_files.extend(list(data_dir.glob(pattern)))\n",
    "        \n",
    "        data_status[data_type] = {\n",
    "            'available': len(found_files) > 0,\n",
    "            'count': len(found_files),\n",
    "            'files': [f.name for f in found_files[:3]]  # Show first 3\n",
    "        }\n",
    "        \n",
    "        status_icon = \"âœ…\" if data_status[data_type]['available'] else \"âŒ\"\n",
    "        print(f\"   {status_icon} {data_type}: {data_status[data_type]['count']} files found\")\n",
    "        \n",
    "        if data_status[data_type]['files']:\n",
    "            print(f\"      ğŸ“„ Examples: {', '.join(data_status[data_type]['files'])}\")\n",
    "    \n",
    "    # Overall status\n",
    "    available_types = sum(1 for status in data_status.values() if status['available'])\n",
    "    total_types = len(data_status)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Data Availability Summary: {available_types}/{total_types} data types available\")\n",
    "    \n",
    "    if available_types == 0:\n",
    "        print(\"\\nğŸ“‹ To add competition data:\")\n",
    "        print(\"   1. Download data from Kaggle competition page\")\n",
    "        print(\"   2. Extract files to appropriate subdirectories:\")\n",
    "        for data_type in expected_files.keys():\n",
    "            print(f\"      - {data_type}: {data_dirs['raw'] / data_type.lower()}\")\n",
    "        print(\"   3. Re-run this notebook to process real data\")\n",
    "        print(\"\\nğŸ”— Competition URL: https://www.kaggle.com/competitions/openai-to-z-challenge/\")\n",
    "    \n",
    "    return data_status\n",
    "\n",
    "# Check data availability\n",
    "data_status = check_competition_data_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Generator\n",
    "\n",
    "Since competition data may not be immediately available, we'll create realistic sample data that follows expected formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_competition_data():\n",
    "    \"\"\"Create realistic sample data that mimics competition format.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ—ï¸  Creating sample competition data...\")\n",
    "    \n",
    "    # Amazon basin coordinates (typical competition area)\n",
    "    amazon_bounds = {\n",
    "        'west': -75.0,\n",
    "        'south': -10.0,\n",
    "        'east': -45.0,\n",
    "        'north': 5.0\n",
    "    }\n",
    "    \n",
    "    # Sample area: 50km x 50km\n",
    "    sample_area = {\n",
    "        'west': -60.5,\n",
    "        'south': -3.5,\n",
    "        'east': -60.0,\n",
    "        'north': -3.0\n",
    "    }\n",
    "    \n",
    "    # Grid parameters\n",
    "    resolution = 0.0001  # ~10m resolution\n",
    "    width = int((sample_area['east'] - sample_area['west']) / resolution)\n",
    "    height = int((sample_area['north'] - sample_area['south']) / resolution)\n",
    "    \n",
    "    print(f\"   ğŸ“ Sample area: {width} x {height} pixels\")\n",
    "    print(f\"   ğŸŒ Coordinates: {sample_area}\")\n",
    "    print(f\"   ğŸ“ Resolution: {resolution}Â° (~10m)\")\n",
    "    \n",
    "    # Create coordinate arrays\n",
    "    x_coords = np.linspace(sample_area['west'], sample_area['east'], width)\n",
    "    y_coords = np.linspace(sample_area['south'], sample_area['north'], height)\n",
    "    X, Y = np.meshgrid(x_coords, y_coords)\n",
    "    \n",
    "    # 1. LiDAR Elevation Data\n",
    "    print(\"   ğŸ”ï¸  Generating LiDAR elevation data...\")\n",
    "    \n",
    "    # Amazon-like elevation (0-200m with river valleys)\n",
    "    base_elevation = 100\n",
    "    elevation = (\n",
    "        base_elevation +\n",
    "        30 * np.sin(X * 500) * np.cos(Y * 400) +  # Large-scale terrain\n",
    "        15 * np.sin(X * 1000) * np.sin(Y * 800) +  # Medium-scale features\n",
    "        5 * np.random.normal(0, 1, (height, width))  # Small-scale noise\n",
    "    )\n",
    "    \n",
    "    # Add river valleys (lower elevation)\n",
    "    river_mask = np.abs(np.sin(X * 800)) < 0.1\n",
    "    elevation[river_mask] -= 20\n",
    "    \n",
    "    elevation = np.clip(elevation, 0, 250).astype(np.float32)\n",
    "    \n",
    "    # 2. Satellite NDVI Data\n",
    "    print(\"   ğŸŒ± Generating satellite NDVI data...\")\n",
    "    \n",
    "    # Amazon vegetation patterns\n",
    "    base_ndvi = 0.85  # Dense forest\n",
    "    ndvi = (\n",
    "        base_ndvi +\n",
    "        0.1 * np.sin(X * 600) * np.cos(Y * 500) +  # Vegetation variations\n",
    "        0.05 * np.random.normal(0, 1, (height, width))  # Natural variation\n",
    "    )\n",
    "    \n",
    "    # Reduce NDVI near rivers (riparian zones)\n",
    "    ndvi[river_mask] *= 0.7\n",
    "    \n",
    "    # Add deforested areas (archaeological signatures)\n",
    "    for _ in range(8):  # 8 potential archaeological areas\n",
    "        center_x = np.random.randint(width//4, 3*width//4)\n",
    "        center_y = np.random.randint(height//4, 3*height//4)\n",
    "        radius = np.random.randint(10, 30)\n",
    "        \n",
    "        # Create circular cleared area\n",
    "        y_indices, x_indices = np.ogrid[:height, :width]\n",
    "        mask = (x_indices - center_x)**2 + (y_indices - center_y)**2 <= radius**2\n",
    "        ndvi[mask] -= np.random.uniform(0.2, 0.4)\n",
    "    \n",
    "    ndvi = np.clip(ndvi, 0, 1).astype(np.float32)\n",
    "    \n",
    "    # 3. Create sample files with proper geospatial metadata\n",
    "    from rasterio.transform import from_bounds\n",
    "    \n",
    "    transform = from_bounds(\n",
    "        sample_area['west'], sample_area['south'], \n",
    "        sample_area['east'], sample_area['north'],\n",
    "        width, height\n",
    "    )\n",
    "    \n",
    "    crs = 'EPSG:4326'  # WGS84\n",
    "    \n",
    "    # Save LiDAR elevation\n",
    "    lidar_path = data_dirs['lidar'] / 'amazon_elevation_sample.tif'\n",
    "    with rasterio.open(\n",
    "        lidar_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height, width=width,\n",
    "        count=1, dtype=elevation.dtype,\n",
    "        crs=crs, transform=transform,\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(elevation, 1)\n",
    "        dst.set_band_description(1, 'Elevation (meters)')\n",
    "    \n",
    "    # Save NDVI\n",
    "    ndvi_path = data_dirs['ndvi'] / 'amazon_ndvi_sample.tif'\n",
    "    with rasterio.open(\n",
    "        ndvi_path, 'w',\n",
    "        driver='GTiff',\n",
    "        height=height, width=width,\n",
    "        count=1, dtype=ndvi.dtype,\n",
    "        crs=crs, transform=transform,\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(ndvi, 1)\n",
    "        dst.set_band_description(1, 'NDVI')\n",
    "    \n",
    "    # 4. Create sample archaeological literature data\n",
    "    print(\"   ğŸ“š Generating archaeological literature data...\")\n",
    "    \n",
    "    literature_data = {\n",
    "        'metadata': {\n",
    "            'source': 'Sample Archaeological Database',\n",
    "            'region': 'Amazon Basin',\n",
    "            'date_compiled': '2025-01-01',\n",
    "            'coordinate_system': 'WGS84'\n",
    "        },\n",
    "        'papers': [\n",
    "            {\n",
    "                'doi': '10.1016/j.sample.2024.001',\n",
    "                'title': 'Pre-Columbian Settlement Patterns in the Upper Amazon',\n",
    "                'authors': ['Smith, J.', 'Garcia, M.', 'Silva, A.'],\n",
    "                'year': 2024,\n",
    "                'coordinates_mentioned': [\n",
    "                    {'latitude': -3.25, 'longitude': -60.25, 'confidence': 0.9},\n",
    "                    {'latitude': -3.15, 'longitude': -60.35, 'confidence': 0.7}\n",
    "                ],\n",
    "                'site_types': ['settlement', 'agricultural'],\n",
    "                'cultural_period': '800-1200 CE'\n",
    "            },\n",
    "            {\n",
    "                'doi': '10.1002/sample.2023.789',\n",
    "                'title': 'LiDAR Detection of Archaeological Features in Amazonia',\n",
    "                'authors': ['Brown, K.', 'Oliveira, P.'],\n",
    "                'year': 2023,\n",
    "                'coordinates_mentioned': [\n",
    "                    {'latitude': -3.35, 'longitude': -60.15, 'confidence': 0.8}\n",
    "                ],\n",
    "                'site_types': ['ceremonial'],\n",
    "                'cultural_period': '1000-1500 CE'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    literature_path = data_dirs['literature'] / 'archaeological_literature.json'\n",
    "    with open(literature_path, 'w') as f:\n",
    "        json.dump(literature_data, f, indent=2)\n",
    "    \n",
    "    # 5. Create sample GIS data (protected areas, rivers)\n",
    "    print(\"   ğŸ—ºï¸  Generating GIS vector data...\")\n",
    "    \n",
    "    # Create sample river line\n",
    "    from shapely.geometry import LineString, Point\n",
    "    \n",
    "    river_coords = [\n",
    "        (sample_area['west'], sample_area['south'] + 0.1),\n",
    "        (sample_area['west'] + 0.2, sample_area['south'] + 0.2),\n",
    "        (sample_area['west'] + 0.4, sample_area['south'] + 0.15),\n",
    "        (sample_area['east'], sample_area['north'] - 0.1)\n",
    "    ]\n",
    "    \n",
    "    gis_data = gpd.GeoDataFrame({\n",
    "        'feature_type': ['river'],\n",
    "        'name': ['Rio Sample'],\n",
    "        'geometry': [LineString(river_coords)]\n",
    "    }, crs='EPSG:4326')\n",
    "    \n",
    "    gis_path = data_dirs['gis'] / 'amazon_features.geojson'\n",
    "    gis_data.to_file(gis_path, driver='GeoJSON')\n",
    "    \n",
    "    print(f\"âœ… Sample data created successfully:\")\n",
    "    print(f\"   ğŸ”ï¸  LiDAR: {lidar_path}\")\n",
    "    print(f\"   ğŸŒ± NDVI: {ndvi_path}\")\n",
    "    print(f\"   ğŸ“š Literature: {literature_path}\")\n",
    "    print(f\"   ğŸ—ºï¸  GIS: {gis_path}\")\n",
    "    \n",
    "    return {\n",
    "        'elevation_path': lidar_path,\n",
    "        'ndvi_path': ndvi_path,\n",
    "        'literature_path': literature_path,\n",
    "        'gis_path': gis_path,\n",
    "        'bounds': sample_area,\n",
    "        'crs': crs,\n",
    "        'resolution': resolution\n",
    "    }\n",
    "\n",
    "# Create sample data if no real data is available\n",
    "if not any(status['available'] for status in data_status.values()):\n",
    "    print(\"ğŸ“‹ No competition data found. Creating sample data for demonstration...\")\n",
    "    sample_data_info = create_sample_competition_data()\nelse:\n",
    "    print(\"âœ… Competition data detected. Will process real data.\")\n",
    "    sample_data_info = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Validation Pipeline\n",
    "\n",
    "Load and validate all available data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIntegrationPipeline:\n",
    "    \"\"\"Comprehensive data integration pipeline for archaeological site detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_directories):\n",
    "        self.data_dirs = data_directories\n",
    "        self.validator = DatasetValidator()\n",
    "        self.loaded_datasets = {}\n",
    "        self.aligned_datasets = {}\n",
    "        \n",
    "    def discover_data_files(self):\n",
    "        \"\"\"Automatically discover data files in directory structure.\"\"\"\n",
    "        \n",
    "        print(\"ğŸ” Discovering data files...\")\n",
    "        \n",
    "        file_patterns = {\n",
    "            'lidar': ['*.tif', '*.tiff', '*.las', '*.laz'],\n",
    "            'satellite': ['*.tif', '*.tiff', '*.jp2'],\n",
    "            'ndvi': ['*.tif', '*.tiff'],\n",
    "            'gis': ['*.shp', '*.geojson', '*.gpkg'],\n",
    "            'literature': ['*.json', '*.csv']\n",
    "        }\n",
    "        \n",
    "        discovered_files = {}\n",
    "        \n",
    "        for data_type, patterns in file_patterns.items():\n",
    "            data_dir = self.data_dirs[data_type]\n",
    "            files = []\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                files.extend(list(data_dir.glob(pattern)))\n",
    "            \n",
    "            discovered_files[data_type] = files\n",
    "            print(f\"   ğŸ“ {data_type}: {len(files)} files found\")\n",
    "            \n",
    "            for file_path in files[:3]:  # Show first 3 files\n",
    "                print(f\"      ğŸ“„ {file_path.name}\")\n",
    "            \n",
    "            if len(files) > 3:\n",
    "                print(f\"      ... and {len(files) - 3} more files\")\n",
    "        \n",
    "        return discovered_files\n",
    "    \n",
    "    def load_raster_data(self, file_path):\n",
    "        \"\"\"Load and validate raster data.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(file_path) as src:\n",
    "                data = src.read(1).astype(np.float32)\n",
    "                metadata = {\n",
    "                    'crs': src.crs,\n",
    "                    'transform': src.transform,\n",
    "                    'bounds': src.bounds,\n",
    "                    'shape': data.shape,\n",
    "                    'dtype': data.dtype,\n",
    "                    'nodata': src.nodata\n",
    "                }\n",
    "                \n",
    "                # Data quality assessment\n",
    "                quality = self.validator.assess_data_quality(data)\n",
    "                \n",
    "                print(f\"   âœ… Loaded {file_path.name}:\")\n",
    "                print(f\"      ğŸ“ Shape: {metadata['shape']}\")\n",
    "                print(f\"      ğŸ—ºï¸  CRS: {metadata['crs']}\")\n",
    "                print(f\"      ğŸ“Š Data range: {data.min():.3f} - {data.max():.3f}\")\n",
    "                print(f\"      âœ… Quality score: {quality.quality_score:.3f}\")\n",
    "                \n",
    "                return {\n",
    "                    'data': data,\n",
    "                    'metadata': metadata,\n",
    "                    'quality': quality,\n",
    "                    'source_path': file_path\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Failed to load {file_path.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_vector_data(self, file_path):\n",
    "        \"\"\"Load and validate vector data.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            gdf = gpd.read_file(file_path)\n",
    "            \n",
    "            print(f\"   âœ… Loaded {file_path.name}:\")\n",
    "            print(f\"      ğŸ“Š Features: {len(gdf)}\")\n",
    "            print(f\"      ğŸ—ºï¸  CRS: {gdf.crs}\")\n",
    "            print(f\"      ğŸ“‹ Columns: {list(gdf.columns)}\")\n",
    "            \n",
    "            return {\n",
    "                'data': gdf,\n",
    "                'metadata': {\n",
    "                    'crs': gdf.crs,\n",
    "                    'bounds': gdf.total_bounds,\n",
    "                    'feature_count': len(gdf)\n",
    "                },\n",
    "                'source_path': file_path\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Failed to load {file_path.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_literature_data(self, file_path):\n",
    "        \"\"\"Load and validate archaeological literature data.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if file_path.suffix == '.json':\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "            elif file_path.suffix == '.csv':\n",
    "                data = pd.read_csv(file_path).to_dict('records')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported format: {file_path.suffix}\")\n",
    "            \n",
    "            # Extract coordinate information\n",
    "            coordinates = []\n",
    "            if isinstance(data, dict) and 'papers' in data:\n",
    "                for paper in data['papers']:\n",
    "                    if 'coordinates_mentioned' in paper:\n",
    "                        coordinates.extend(paper['coordinates_mentioned'])\n",
    "            \n",
    "            print(f\"   âœ… Loaded {file_path.name}:\")\n",
    "            if isinstance(data, dict) and 'papers' in data:\n",
    "                print(f\"      ğŸ“š Papers: {len(data['papers'])}\")\n",
    "            print(f\"      ğŸ“ Coordinates: {len(coordinates)}\")\n",
    "            \n",
    "            return {\n",
    "                'data': data,\n",
    "                'coordinates': coordinates,\n",
    "                'source_path': file_path\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Failed to load {file_path.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all discovered data files.\"\"\"\n",
    "        \n",
    "        print(\"ğŸ“¦ Loading all data files...\")\n",
    "        \n",
    "        discovered_files = self.discover_data_files()\n",
    "        \n",
    "        # Load LiDAR data\n",
    "        if discovered_files['lidar']:\n",
    "            print(\"\\nğŸ”ï¸  Loading LiDAR data...\")\n",
    "            self.loaded_datasets['lidar'] = []\n",
    "            for file_path in discovered_files['lidar']:\n",
    "                dataset = self.load_raster_data(file_path)\n",
    "                if dataset:\n",
    "                    self.loaded_datasets['lidar'].append(dataset)\n",
    "        \n",
    "        # Load NDVI data\n",
    "        if discovered_files['ndvi']:\n",
    "            print(\"\\nğŸŒ± Loading NDVI data...\")\n",
    "            self.loaded_datasets['ndvi'] = []\n",
    "            for file_path in discovered_files['ndvi']:\n",
    "                dataset = self.load_raster_data(file_path)\n",
    "                if dataset:\n",
    "                    self.loaded_datasets['ndvi'].append(dataset)\n",
    "        \n",
    "        # Load satellite data\n",
    "        if discovered_files['satellite']:\n",
    "            print(\"\\nğŸ›°ï¸  Loading satellite data...\")\n",
    "            self.loaded_datasets['satellite'] = []\n",
    "            for file_path in discovered_files['satellite']:\n",
    "                dataset = self.load_raster_data(file_path)\n",
    "                if dataset:\n",
    "                    self.loaded_datasets['satellite'].append(dataset)\n",
    "        \n",
    "        # Load GIS data\n",
    "        if discovered_files['gis']:\n",
    "            print(\"\\nğŸ—ºï¸  Loading GIS data...\")\n",
    "            self.loaded_datasets['gis'] = []\n",
    "            for file_path in discovered_files['gis']:\n",
    "                dataset = self.load_vector_data(file_path)\n",
    "                if dataset:\n",
    "                    self.loaded_datasets['gis'].append(dataset)\n",
    "        \n",
    "        # Load literature data\n",
    "        if discovered_files['literature']:\n",
    "            print(\"\\nğŸ“š Loading literature data...\")\n",
    "            self.loaded_datasets['literature'] = []\n",
    "            for file_path in discovered_files['literature']:\n",
    "                dataset = self.load_literature_data(file_path)\n",
    "                if dataset:\n",
    "                    self.loaded_datasets['literature'].append(dataset)\n",
    "        \n",
    "        print(f\"\\nâœ… Data loading complete:\")\n",
    "        for data_type, datasets in self.loaded_datasets.items():\n",
    "            print(f\"   ğŸ“Š {data_type}: {len(datasets)} datasets loaded\")\n",
    "        \n",
    "        return self.loaded_datasets\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DataIntegrationPipeline(data_dirs)\n",
    "\n",
    "# Load all available data\n",
    "loaded_data = pipeline.load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Coordinate System Alignment\n",
    "\n",
    "Align all datasets to a common coordinate reference system and spatial extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_datasets_to_common_grid(loaded_datasets, target_crs='EPSG:4326', target_resolution=0.0001):\n",
    "    \"\"\"Align all raster datasets to a common grid.\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ”„ Aligning datasets to common grid...\")\n",
    "    print(f\"   ğŸ¯ Target CRS: {target_crs}\")\n",
    "    print(f\"   ğŸ“ Target resolution: {target_resolution}Â° (~10m)\")\n",
    "    \n",
    "    # Collect all raster datasets\n",
    "    raster_datasets = []\n",
    "    for data_type in ['lidar', 'ndvi', 'satellite']:\n",
    "        if data_type in loaded_datasets:\n",
    "            raster_datasets.extend(loaded_datasets[data_type])\n",
    "    \n",
    "    if not raster_datasets:\n",
    "        print(\"âŒ No raster datasets found for alignment\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"   ğŸ“Š Aligning {len(raster_datasets)} raster datasets\")\n",
    "    \n",
    "    # Find common bounds\n",
    "    all_bounds = []\n",
    "    for dataset in raster_datasets:\n",
    "        bounds = dataset['metadata']['bounds']\n",
    "        \n",
    "        # Transform bounds to target CRS if needed\n",
    "        src_crs = dataset['metadata']['crs']\n",
    "        if src_crs != target_crs:\n",
    "            transformer = CoordinateTransformer(str(src_crs), target_crs)\n",
    "            transformed_bounds = transformer.transform_bounds(bounds)\n",
    "            all_bounds.append(transformed_bounds)\n",
    "        else:\n",
    "            all_bounds.append(bounds)\n",
    "    \n",
    "    # Calculate intersection of all bounds\n",
    "    common_bounds = (\n",
    "        max(bounds[0] for bounds in all_bounds),  # west\n",
    "        max(bounds[1] for bounds in all_bounds),  # south\n",
    "        min(bounds[2] for bounds in all_bounds),  # east\n",
    "        min(bounds[3] for bounds in all_bounds)   # north\n",
    "    )\n",
    "    \n",
    "    print(f\"   ğŸŒ Common bounds: {common_bounds}\")\n",
    "    \n",
    "    # Check if common bounds are valid\n",
    "    if common_bounds[0] >= common_bounds[2] or common_bounds[1] >= common_bounds[3]:\n",
    "        print(\"âŒ No spatial overlap between datasets\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate common grid dimensions\n",
    "    width = int((common_bounds[2] - common_bounds[0]) / target_resolution)\n",
    "    height = int((common_bounds[3] - common_bounds[1]) / target_resolution)\n",
    "    \n",
    "    print(f\"   ğŸ“ Common grid: {width} x {height} pixels\")\n",
    "    \n",
    "    # Create common transform\n",
    "    from rasterio.transform import from_bounds\n",
    "    common_transform = from_bounds(\n",
    "        common_bounds[0], common_bounds[1],\n",
    "        common_bounds[2], common_bounds[3],\n",
    "        width, height\n",
    "    )\n",
    "    \n",
    "    aligned_datasets = {}\n",
    "    \n",
    "    # Align each dataset\n",
    "    for i, dataset in enumerate(raster_datasets):\n",
    "        print(f\"   ğŸ“Š Aligning dataset {i+1}/{len(raster_datasets)}...\")\n",
    "        \n",
    "        src_data = dataset['data']\n",
    "        src_transform = dataset['metadata']['transform']\n",
    "        src_crs = dataset['metadata']['crs']\n",
    "        \n",
    "        # Create output array\n",
    "        aligned_data = np.zeros((height, width), dtype=np.float32)\n",
    "        \n",
    "        # Reproject to common grid\n",
    "        reproject(\n",
    "            source=src_data,\n",
    "            destination=aligned_data,\n",
    "            src_transform=src_transform,\n",
    "            src_crs=src_crs,\n",
    "            dst_transform=common_transform,\n",
    "            dst_crs=target_crs,\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "        \n",
    "        # Store aligned dataset\n",
    "        aligned_key = f\"aligned_{i}\"\n",
    "        aligned_datasets[aligned_key] = {\n",
    "            'data': aligned_data,\n",
    "            'metadata': {\n",
    "                'crs': target_crs,\n",
    "                'transform': common_transform,\n",
    "                'bounds': common_bounds,\n",
    "                'shape': (height, width)\n",
    "            },\n",
    "            'original_dataset': dataset\n",
    "        }\n",
    "    \n",
    "    print(f\"âœ… Dataset alignment complete: {len(aligned_datasets)} datasets aligned\")\n",
    "    \n",
    "    return aligned_datasets\n",
    "\n",
    "# Align datasets if we have raster data\n",
    "if any(data_type in loaded_data for data_type in ['lidar', 'ndvi', 'satellite']):\n",
    "    aligned_data = align_datasets_to_common_grid(loaded_data)\n",
    "    \n",
    "    if aligned_data:\n",
    "        # Save aligned datasets\n",
    "        print(\"\\nğŸ’¾ Saving aligned datasets...\")\n",
    "        \n",
    "        for key, dataset in aligned_data.items():\n",
    "            output_path = data_dirs['aligned'] / f'{key}.tif'\n",
    "            \n",
    "            with rasterio.open(\n",
    "                output_path, 'w',\n",
    "                driver='GTiff',\n",
    "                height=dataset['metadata']['shape'][0],\n",
    "                width=dataset['metadata']['shape'][1],\n",
    "                count=1,\n",
    "                dtype=dataset['data'].dtype,\n",
    "                crs=dataset['metadata']['crs'],\n",
    "                transform=dataset['metadata']['transform'],\n",
    "                compress='lzw'\n",
    "            ) as dst:\n",
    "                dst.write(dataset['data'], 1)\n",
    "            \n",
    "            print(f\"   ğŸ’¾ Saved: {output_path.name}\")\n",
    "    \nelse:\n",
    "    print(\"âš ï¸  No raster datasets available for alignment\")\n",
    "    aligned_data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization & Quality Assessment\n",
    "\n",
    "Visualize loaded and aligned data for quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_integrated_data(aligned_data, loaded_data):\n",
    "    \"\"\"Visualize integrated datasets for quality assessment.\"\"\"\n",
    "    \n",
    "    if not aligned_data:\n",
    "        print(\"âš ï¸  No aligned data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š Creating data visualization...\")\n",
    "    \n",
    "    # Determine number of datasets to visualize\n",
    "    n_datasets = len(aligned_data)\n",
    "    \n",
    "    if n_datasets == 0:\n",
    "        print(\"âŒ No datasets to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create subplot layout\n",
    "    cols = min(3, n_datasets)\n",
    "    rows = (n_datasets + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    \n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Plot each aligned dataset\n",
    "    for i, (key, dataset) in enumerate(aligned_data.items()):\n",
    "        data = dataset['data']\n",
    "        original = dataset['original_dataset']\n",
    "        \n",
    "        # Determine data type and color scheme\n",
    "        source_name = original['source_path'].stem\n",
    "        if 'elevation' in source_name.lower() or 'lidar' in source_name.lower():\n",
    "            cmap = 'terrain'\n",
    "            title = f'Elevation Data\\n{source_name}'\n",
    "            unit = 'm'\n",
    "        elif 'ndvi' in source_name.lower():\n",
    "            cmap = 'RdYlGn'\n",
    "            title = f'NDVI Data\\n{source_name}'\n",
    "            unit = ''\n",
    "        else:\n",
    "            cmap = 'viridis'\n",
    "            title = f'Satellite Data\\n{source_name}'\n",
    "            unit = ''\n",
    "        \n",
    "        # Plot data\n",
    "        im = axes[i].imshow(data, cmap=cmap, aspect='equal')\n",
    "        axes[i].set_title(title, fontweight='bold', fontsize=10)\n",
    "        axes[i].set_xlabel('X Coordinate')\n",
    "        axes[i].set_ylabel('Y Coordinate')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[i], shrink=0.8, label=unit)\n",
    "        \n",
    "        # Add data statistics\n",
    "        stats_text = f'Range: {data.min():.2f} - {data.max():.2f}\\nMean: {data.mean():.2f}'\n",
    "        axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, \n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                    fontsize=8)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(aligned_data), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show spatial overlap information\n",
    "    if aligned_data:\n",
    "        sample_dataset = next(iter(aligned_data.values()))\n",
    "        bounds = sample_dataset['metadata']['bounds']\n",
    "        shape = sample_dataset['metadata']['shape']\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Aligned Data Summary:\")\n",
    "        print(f\"   ğŸŒ Spatial Extent: {bounds[0]:.4f}, {bounds[1]:.4f} to {bounds[2]:.4f}, {bounds[3]:.4f}\")\n",
    "        print(f\"   ğŸ“ Grid Size: {shape[1]} x {shape[0]} pixels\")\n",
    "        print(f\"   ğŸ“ Coverage: {(bounds[2] - bounds[0]):.4f}Â° x {(bounds[3] - bounds[1]):.4f}Â°\")\n",
    "        print(f\"   ğŸ¯ Pixel Size: ~{(bounds[2] - bounds[0]) / shape[1] * 111:.0f}m x {(bounds[3] - bounds[1]) / shape[0] * 111:.0f}m\")\n",
    "    \n",
    "    # Literature data summary\n",
    "    if 'literature' in loaded_data:\n",
    "        print(f\"\\nğŸ“š Literature Data Summary:\")\n",
    "        total_coords = 0\n",
    "        for lit_dataset in loaded_data['literature']:\n",
    "            coords = lit_dataset['coordinates']\n",
    "            total_coords += len(coords)\n",
    "            print(f\"   ğŸ“ {len(coords)} coordinate references found\")\n",
    "            \n",
    "            # Show sample coordinates\n",
    "            for coord in coords[:3]:\n",
    "                print(f\"      ğŸ“Œ ({coord['latitude']:.3f}, {coord['longitude']:.3f}) - confidence: {coord['confidence']:.2f}\")\n",
    "        \n",
    "        print(f\"   ğŸ“Š Total literature coordinates: {total_coords}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Data visualization complete!\")\n",
    "\n",
    "# Visualize the integrated data\n",
    "visualize_integrated_data(aligned_data, loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction Pipeline\n",
    "\n",
    "Extract features from integrated datasets for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_aligned_data(aligned_data):\n",
    "    \"\"\"Extract comprehensive features from aligned datasets.\"\"\"\n",
    "    \n",
    "    if not aligned_data:\n",
    "        print(\"âš ï¸  No aligned data available for feature extraction\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ” Extracting features from aligned datasets...\")\n",
    "    \n",
    "    # Initialize feature extraction components\n",
    "    from geospatial_processing import TerrainAnalyzer, VegetationAnalyzer, SpatialFeatureExtractor\n",
    "    \n",
    "    terrain_analyzer = TerrainAnalyzer()\n",
    "    vegetation_analyzer = VegetationAnalyzer()\n",
    "    feature_extractor = SpatialFeatureExtractor()\n",
    "    \n",
    "    # Identify data types from aligned datasets\n",
    "    elevation_data = None\n",
    "    ndvi_data = None\n",
    "    satellite_data = None\n",
    "    \n",
    "    for key, dataset in aligned_data.items():\n",
    "        source_name = dataset['original_dataset']['source_path'].stem.lower()\n",
    "        data = dataset['data']\n",
    "        \n",
    "        if 'elevation' in source_name or 'lidar' in source_name:\n",
    "            elevation_data = data\n",
    "            print(f\"   ğŸ”ï¸  Elevation data identified: {source_name}\")\n",
    "        elif 'ndvi' in source_name:\n",
    "            ndvi_data = data\n",
    "            print(f\"   ğŸŒ± NDVI data identified: {source_name}\")\n",
    "        else:\n",
    "            satellite_data = data\n",
    "            print(f\"   ğŸ›°ï¸  Satellite data identified: {source_name}\")\n",
    "    \n",
    "    extracted_features = {}\n",
    "    \n",
    "    # Extract terrain features from elevation data\n",
    "    if elevation_data is not None:\n",
    "        print(f\"   ğŸ”ï¸  Extracting terrain features...\")\n",
    "        \n",
    "        pixel_size = 10  # Approximate 10m resolution\n",
    "        \n",
    "        slope = terrain_analyzer.calculate_slope(elevation_data, pixel_size)\n",
    "        aspect = terrain_analyzer.calculate_aspect(elevation_data, pixel_size)\n",
    "        curvature = terrain_analyzer.calculate_curvature(elevation_data, pixel_size)\n",
    "        terrain_features = terrain_analyzer.identify_terrain_features(elevation_data)\n",
    "        \n",
    "        extracted_features.update({\n",
    "            'elevation': elevation_data,\n",
    "            'slope': slope,\n",
    "            'aspect': aspect,\n",
    "            'profile_curvature': curvature['profile_curvature'],\n",
    "            'plan_curvature': curvature['plan_curvature'],\n",
    "            'terrain_features': terrain_features\n",
    "        })\n",
    "        \n",
    "        print(f\"      âœ… Terrain features extracted: elevation, slope, aspect, curvature\")\n",
    "        print(f\"      ğŸ—» Terrain features identified: {len(terrain_features['peaks'])} peaks, {len(terrain_features['valleys'])} valleys\")\n",
    "    \n",
    "    # Extract vegetation features from NDVI data\n",
    "    if ndvi_data is not None:\n",
    "        print(f\"   ğŸŒ± Extracting vegetation features...\")\n",
    "        \n",
    "        vegetation_classes = vegetation_analyzer.classify_vegetation_density(ndvi_data)\n",
    "        vegetation_anomalies = vegetation_analyzer.detect_vegetation_anomalies(\n",
    "            ndvi_data, window_size=3, threshold=0.1\n",
    "        )\n",
    "        \n",
    "        # Create synthetic NIR and Red bands for additional indices\n",
    "        red_band = np.random.uniform(0.1, 0.3, ndvi_data.shape)\n",
    "        nir_band = red_band * (1 + ndvi_data) / (1 - ndvi_data + 1e-6)  # Correctly derive NIR from NDVI formula: NIR = Red * (1 + NDVI) / (1 - NDVI)\n",
    "        vegetation_indices = vegetation_analyzer.calculate_vegetation_indices(red_band, nir_band)\n",
    "        \n",
    "        extracted_features.update({\n",
    "            'ndvi': ndvi_data,\n",
    "            'vegetation_classes': vegetation_classes,\n",
    "            'vegetation_anomalies': vegetation_anomalies.astype(float),\n",
    "            'savi': vegetation_indices['savi'],\n",
    "            'evi': vegetation_indices['evi']\n",
    "        })\n",
    "        \n",
    "        print(f\"      âœ… Vegetation features extracted: NDVI, classes, anomalies, SAVI, EVI\")\n",
    "        print(f\"      âš ï¸  Vegetation anomalies: {np.sum(vegetation_anomalies)} pixels ({np.sum(vegetation_anomalies)/vegetation_anomalies.size*100:.2f}%)\")\n",
    "    \n",
    "    # Extract spatial/textural features\n",
    "    if elevation_data is not None:\n",
    "        print(f\"   ğŸ—ï¸  Extracting spatial features...\")\n",
    "        \n",
    "        # Sample for textural analysis (computationally intensive)\n",
    "        step = max(1, elevation_data.shape[0] // 200)  # Sample to ~200x200 for speed\n",
    "        elevation_sample = elevation_data[::step, ::step]\n",
    "        \n",
    "        textural_features_sample = feature_extractor.extract_textural_features(\n",
    "            elevation_sample, window_size=3\n",
    "        )\n",
    "        \n",
    "        # Upsample back to original size\n",
    "        from scipy.ndimage import zoom\n",
    "        zoom_factor = (elevation_data.shape[0] / textural_features_sample['contrast'].shape[0],\n",
    "                      elevation_data.shape[1] / textural_features_sample['contrast'].shape[1])\n",
    "        \n",
    "        textural_features = {}\n",
    "        for key, feature in textural_features_sample.items():\n",
    "            textural_features[key] = zoom(feature, zoom_factor, order=1)\n",
    "        \n",
    "        extracted_features.update({\n",
    "            'texture_contrast': textural_features['contrast'],\n",
    "            'texture_homogeneity': textural_features['homogeneity'],\n",
    "            'texture_energy': textural_features['energy'],\n",
    "            'texture_dissimilarity': textural_features['dissimilarity']\n",
    "        })\n",
    "        \n",
    "        print(f\"      âœ… Spatial features extracted: {list(textural_features.keys())}\")\n",
    "    \n",
    "    # Create composite features\n",
    "    if elevation_data is not None and ndvi_data is not None:\n",
    "        print(f\"   ğŸ¯ Creating composite features...\")\n",
    "        \n",
    "        # Archaeological preference score\n",
    "        archaeological_preference = (\n",
    "            np.exp(-((elevation_data - 150)**2) / (2 * 75**2)) *  # Elevation preference\n",
    "            np.exp(-(slope**2) / (2 * 10**2)) *  # Slope preference\n",
    "            (1 + extracted_features['texture_contrast'] / extracted_features['texture_contrast'].max())  # Terrain distinctiveness\n",
    "        )\n",
    "        \n",
    "        # Terrain roughness\n",
    "        terrain_roughness = np.sqrt(\n",
    "            slope**2 + \n",
    "            extracted_features['profile_curvature']**2 + \n",
    "            extracted_features['plan_curvature']**2\n",
    "        )\n",
    "        \n",
    "        # Vegetation health\n",
    "        vegetation_health = ndvi_data * (1 - extracted_features['vegetation_anomalies'])\n",
    "        \n",
    "        extracted_features.update({\n",
    "            'archaeological_preference': archaeological_preference,\n",
    "            'terrain_roughness': terrain_roughness,\n",
    "            'vegetation_health': vegetation_health\n",
    "        })\n",
    "        \n",
    "        print(f\"      âœ… Composite features created: archaeological_preference, terrain_roughness, vegetation_health\")\n",
    "    \n",
    "    print(f\"\\nâœ… Feature extraction complete:\")\n",
    "    print(f\"   ğŸ“Š Total features: {len(extracted_features)}\")\n",
    "    print(f\"   ğŸ“‹ Feature list: {list(extracted_features.keys())}\")\n",
    "    \n",
    "    # Save features to disk\n",
    "    print(f\"\\nğŸ’¾ Saving extracted features...\")\n",
    "    \n",
    "    for feature_name, feature_data in extracted_features.items():\n",
    "        if isinstance(feature_data, np.ndarray):\n",
    "            # Save as GeoTIFF with same georeferencing as aligned data\n",
    "            sample_dataset = next(iter(aligned_data.values()))\n",
    "            metadata = sample_dataset['metadata']\n",
    "            \n",
    "            output_path = data_dirs['features'] / f'{feature_name}.tif'\n",
    "            \n",
    "            with rasterio.open(\n",
    "                output_path, 'w',\n",
    "                driver='GTiff',\n",
    "                height=metadata['shape'][0],\n",
    "                width=metadata['shape'][1],\n",
    "                count=1,\n",
    "                dtype=feature_data.dtype,\n",
    "                crs=metadata['crs'],\n",
    "                transform=metadata['transform'],\n",
    "                compress='lzw'\n",
    "            ) as dst:\n",
    "                dst.write(feature_data, 1)\n",
    "                dst.set_band_description(1, feature_name)\n",
    "            \n",
    "            print(f\"   ğŸ’¾ {feature_name}: {output_path.name}\")\n",
    "    \n",
    "    return extracted_features\n",
    "\n",
    "# Extract features from aligned data\n",
    "if aligned_data:\n",
    "    extracted_features = extract_features_from_aligned_data(aligned_data)\nelse:\n",
    "    print(\"âš ï¸  No aligned data available for feature extraction\")\n",
    "    extracted_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with Literature Data\n",
    "\n",
    "Integrate archaeological literature coordinates with spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_literature_coordinates(extracted_features, loaded_data, aligned_data):\n",
    "    \"\"\"Integrate archaeological literature coordinates with spatial features.\"\"\"\n",
    "    \n",
    "    if 'literature' not in loaded_data or not loaded_data['literature']:\n",
    "        print(\"âš ï¸  No literature data available for integration\")\n",
    "        return None\n",
    "    \n",
    "    if not extracted_features or not aligned_data:\n",
    "        print(\"âš ï¸  No spatial features available for literature integration\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ“š Integrating literature coordinates with spatial data...\")\n",
    "    \n",
    "    # Get spatial reference information\n",
    "    sample_dataset = next(iter(aligned_data.values()))\n",
    "    bounds = sample_dataset['metadata']['bounds']\n",
    "    transform = sample_dataset['metadata']['transform']\n",
    "    shape = sample_dataset['metadata']['shape']\n",
    "    \n",
    "    print(f\"   ğŸŒ Spatial bounds: {bounds}\")\n",
    "    print(f\"   ğŸ“ Grid shape: {shape}\")\n",
    "    \n",
    "    # Collect all literature coordinates\n",
    "    all_literature_coords = []\n",
    "    \n",
    "    for lit_dataset in loaded_data['literature']:\n",
    "        coords = lit_dataset['coordinates']\n",
    "        all_literature_coords.extend(coords)\n",
    "    \n",
    "    print(f\"   ğŸ“ Total literature coordinates: {len(all_literature_coords)}\")\n",
    "    \n",
    "    # Filter coordinates within spatial bounds\n",
    "    valid_coords = []\n",
    "    \n",
    "    for coord in all_literature_coords:\n",
    "        lat, lon = coord['latitude'], coord['longitude']\n",
    "        \n",
    "        if (bounds[0] <= lon <= bounds[2] and \n",
    "            bounds[1] <= lat <= bounds[3]):\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            col = int((lon - bounds[0]) / (bounds[2] - bounds[0]) * shape[1])\n",
    "            row = int((bounds[3] - lat) / (bounds[3] - bounds[1]) * shape[0])\n",
    "            \n",
    "            # Ensure within bounds\n",
    "            if 0 <= row < shape[0] and 0 <= col < shape[1]:\n",
    "                coord_info = {\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'pixel_row': row,\n",
    "                    'pixel_col': col,\n",
    "                    'confidence': coord['confidence']\n",
    "                }\n",
    "                valid_coords.append(coord_info)\n",
    "    \n",
    "    print(f\"   âœ… Coordinates within study area: {len(valid_coords)}\")\n",
    "    \n",
    "    if not valid_coords:\n",
    "        print(\"   âš ï¸  No literature coordinates overlap with spatial data\")\n",
    "        return None\n",
    "    \n",
    "    # Extract feature values at literature coordinate locations\n",
    "    literature_features = []\n",
    "    \n",
    "    for coord in valid_coords:\n",
    "        row, col = coord['pixel_row'], coord['pixel_col']\n",
    "        \n",
    "        feature_values = {\n",
    "            'latitude': coord['latitude'],\n",
    "            'longitude': coord['longitude'],\n",
    "            'confidence': coord['confidence'],\n",
    "            'pixel_row': row,\n",
    "            'pixel_col': col\n",
    "        }\n",
    "        \n",
    "        # Extract feature values at this location\n",
    "        for feature_name, feature_data in extracted_features.items():\n",
    "            if isinstance(feature_data, np.ndarray) and feature_data.ndim == 2:\n",
    "                feature_values[feature_name] = float(feature_data[row, col])\n",
    "        \n",
    "        literature_features.append(feature_values)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    literature_df = pd.DataFrame(literature_features)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Literature-Spatial Integration Summary:\")\n",
    "    print(f\"   ğŸ“š Literature sites in study area: {len(literature_df)}\")\n",
    "    print(f\"   ğŸ“‹ Features extracted per site: {len([col for col in literature_df.columns if col not in ['latitude', 'longitude', 'confidence', 'pixel_row', 'pixel_col']])}\")\n",
    "    \n",
    "    # Show feature statistics at literature sites\n",
    "    if 'elevation' in literature_df.columns:\n",
    "        print(f\"   ğŸ”ï¸  Elevation range at sites: {literature_df['elevation'].min():.1f} - {literature_df['elevation'].max():.1f}m\")\n",
    "    if 'ndvi' in literature_df.columns:\n",
    "        print(f\"   ğŸŒ± NDVI range at sites: {literature_df['ndvi'].min():.3f} - {literature_df['ndvi'].max():.3f}\")\n",
    "    if 'slope' in literature_df.columns:\n",
    "        print(f\"   ğŸ“ Slope range at sites: {literature_df['slope'].min():.1f} - {literature_df['slope'].max():.1f}Â°\")\n",
    "    \n",
    "    # Display sample of literature sites\n",
    "    print(f\"\\nğŸ“‹ Sample Literature Sites:\")\n",
    "    display_cols = ['latitude', 'longitude', 'confidence']\n",
    "    if 'elevation' in literature_df.columns:\n",
    "        display_cols.append('elevation')\n",
    "    if 'ndvi' in literature_df.columns:\n",
    "        display_cols.append('ndvi')\n",
    "    \n",
    "    sample_df = literature_df[display_cols].head()\n",
    "    print(sample_df.round(3).to_string(index=False))\n",
    "    \n",
    "    # Save literature integration results\n",
    "    output_path = data_dirs['processed'] / 'literature_spatial_integration.csv'\n",
    "    literature_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nğŸ’¾ Literature integration saved: {output_path.name}\")\n",
    "    \n",
    "    return literature_df\n",
    "\n",
    "# Integrate literature data\n",
    "if extracted_features and 'literature' in loaded_data:\n",
    "    literature_integration = integrate_literature_coordinates(\n",
    "        extracted_features, loaded_data, aligned_data\n",
    "    )\nelse:\n",
    "    print(\"âš ï¸  Cannot integrate literature data: missing features or literature data\")\n",
    "    literature_integration = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Integration Summary & Export\n",
    "\n",
    "Summarize the data integration pipeline and prepare for ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary of data integration pipeline\n",
    "print(\"ğŸ“‹ DATA INTEGRATION PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“ DIRECTORY STRUCTURE:\")\n",
    "for name, path in data_dirs.items():\n",
    "    file_count = len(list(path.glob('*'))) if path.exists() else 0\n",
    "    print(f\"   ğŸ“‚ {name}: {path} ({file_count} files)\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ DATA LOADING RESULTS:\")\n",
    "total_datasets = 0\n",
    "for data_type, datasets in loaded_data.items():\n",
    "    count = len(datasets) if datasets else 0\n",
    "    total_datasets += count\n",
    "    status_icon = \"âœ…\" if count > 0 else \"âŒ\"\n",
    "    print(f\"   {status_icon} {data_type}: {count} datasets loaded\")\n",
    "\n",
    "print(f\"   ğŸ“Š Total datasets loaded: {total_datasets}\")\n",
    "\n",
    "print(f\"\\nğŸ”„ DATA ALIGNMENT:\")\n",
    "if aligned_data:\n",
    "    sample_dataset = next(iter(aligned_data.values()))\n",
    "    bounds = sample_dataset['metadata']['bounds']\n",
    "    shape = sample_dataset['metadata']['shape']\n",
    "    \n",
    "    print(f\"   âœ… Alignment successful: {len(aligned_data)} datasets aligned\")\n",
    "    print(f\"   ğŸŒ Common extent: {bounds[0]:.4f}, {bounds[1]:.4f} to {bounds[2]:.4f}, {bounds[3]:.4f}\")\n",
    "    print(f\"   ğŸ“ Grid dimensions: {shape[1]} x {shape[0]} pixels\")\n",
    "    print(f\"   ğŸ“ Approximate resolution: {(bounds[2] - bounds[0]) / shape[1] * 111:.0f}m x {(bounds[3] - bounds[1]) / shape[0] * 111:.0f}m\")\n",
    "    print(f\"   ğŸ¯ CRS: {sample_dataset['metadata']['crs']}\")\nelse:\n",
    "    print(f\"   âŒ No raster data alignment performed\")\n",
    "\n",
    "print(f\"\\nğŸ” FEATURE EXTRACTION:\")\n",
    "if extracted_features:\n",
    "    print(f\"   âœ… Feature extraction successful: {len(extracted_features)} features\")\n",
    "    \n",
    "    # Categorize features\n",
    "    terrain_features = [f for f in extracted_features.keys() if any(t in f for t in ['elevation', 'slope', 'aspect', 'curvature'])]\n",
    "    vegetation_features = [f for f in extracted_features.keys() if any(v in f for v in ['ndvi', 'vegetation', 'savi', 'evi'])]\n",
    "    texture_features = [f for f in extracted_features.keys() if 'texture' in f]\n",
    "    composite_features = [f for f in extracted_features.keys() if any(c in f for c in ['archaeological', 'roughness', 'health'])]\n",
    "    \n",
    "    print(f\"   ğŸ”ï¸  Terrain features ({len(terrain_features)}): {', '.join(terrain_features)}\")\n",
    "    print(f\"   ğŸŒ± Vegetation features ({len(vegetation_features)}): {', '.join(vegetation_features)}\")\n",
    "    print(f\"   ğŸ—ï¸  Texture features ({len(texture_features)}): {', '.join(texture_features)}\")\n",
    "    print(f\"   ğŸ¯ Composite features ({len(composite_features)}): {', '.join(composite_features)}\")\nelse:\n",
    "    print(f\"   âŒ No features extracted\")\n",
    "\n",
    "print(f\"\\nğŸ“š LITERATURE INTEGRATION:\")\n",
    "if literature_integration is not None:\n",
    "    print(f\"   âœ… Literature integration successful: {len(literature_integration)} sites\")\n",
    "    print(f\"   ğŸ“ Coordinate range:\")\n",
    "    print(f\"      Latitude: {literature_integration['latitude'].min():.4f} to {literature_integration['latitude'].max():.4f}\")\n",
    "    print(f\"      Longitude: {literature_integration['longitude'].min():.4f} to {literature_integration['longitude'].max():.4f}\")\n",
    "    print(f\"   ğŸ¯ Confidence range: {literature_integration['confidence'].min():.2f} to {literature_integration['confidence'].max():.2f}\")\nelse:\n",
    "    print(f\"   âŒ No literature integration performed\")\n",
    "\n",
    "print(f\"\\nğŸ“Š DATA QUALITY ASSESSMENT:\")\n",
    "if aligned_data and extracted_features:\n",
    "    # Basic quality metrics\n",
    "    valid_pixels = 0\n",
    "    total_pixels = 0\n",
    "    \n",
    "    for feature_name, feature_data in extracted_features.items():\n",
    "        if isinstance(feature_data, np.ndarray) and feature_data.ndim == 2:\n",
    "            total_pixels = feature_data.size\n",
    "            valid_pixels = np.sum(~np.isnan(feature_data) & ~np.isinf(feature_data))\n",
    "            break\n",
    "    \n",
    "    if total_pixels > 0:\n",
    "        completeness = valid_pixels / total_pixels * 100\n",
    "        print(f\"   âœ… Data completeness: {completeness:.1f}% ({valid_pixels}/{total_pixels} valid pixels)\")\n",
    "    \n",
    "    print(f\"   ğŸ¯ Ready for machine learning model training\")\nelse:\n",
    "    print(f\"   âš ï¸  Limited data available for quality assessment\")\n",
    "\n",
    "print(f\"\\nğŸš€ NEXT STEPS:\")\n",
    "print(f\"   1. ğŸ¤– Load data in machine learning notebooks\")\n",
    "print(f\"   2. ğŸ“Š Create training/validation datasets\")\n",
    "print(f\"   3. ğŸ¯ Train archaeological site detection models\")\n",
    "print(f\"   4. âœ… Validate predictions against literature sites\")\n",
    "print(f\"   5. ğŸ“‹ Generate competition submission\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ OUTPUT FILES:\")\n",
    "output_files = {\n",
    "    'aligned_data': list(data_dirs['aligned'].glob('*.tif')),\n",
    "    'features': list(data_dirs['features'].glob('*.tif')),\n",
    "    'integration': list(data_dirs['processed'].glob('*.csv'))\n",
    "}\n",
    "\n",
    "for category, files in output_files.items():\n",
    "    print(f\"   ğŸ“ {category}: {len(files)} files\")\n",
    "    for file_path in files[:3]:\n",
    "        print(f\"      ğŸ“„ {file_path.name}\")\n",
    "    if len(files) > 3:\n",
    "        print(f\"      ... and {len(files) - 3} more files\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ DATA INTEGRATION PIPELINE COMPLETED SUCCESSFULLY âœ…\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create pipeline status report\n",
    "pipeline_status = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'total_datasets_loaded': total_datasets,\n",
    "    'aligned_datasets': len(aligned_data) if aligned_data else 0,\n",
    "    'extracted_features': len(extracted_features) if extracted_features else 0,\n",
    "    'literature_sites': len(literature_integration) if literature_integration is not None else 0,\n",
    "    'spatial_bounds': bounds if aligned_data else None,\n",
    "    'grid_shape': shape if aligned_data else None,\n",
    "    'ready_for_ml': bool(extracted_features and aligned_data)\n",
    "}\n",
    "\n",
    "# Save pipeline status\n",
    "status_path = data_dirs['processed'] / 'pipeline_status.json'\n",
    "with open(status_path, 'w') as f:\n",
    "    json.dump(pipeline_status, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Pipeline status saved: {status_path.name}\")\n",
    "print(f\"âœ… Data integration pipeline ready for machine learning workflows!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}